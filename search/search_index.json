{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"MLOps Workshop - Home","text":"<p>Welcome to the MLOps workshop. Use the top navigation menu to enter each section.</p> <p>Info</p> <p>Ask your instructor for the link to the lab environment, and your credentials to access it.</p>"},{"location":"assets/","title":"Assets used in the workshop","text":"<p>IMPORTANT</p> <p>Download the following assets, as you will use them in the labs.</p>"},{"location":"assets/#data-assets","title":"Data Assets","text":"<ul> <li>Customer Churn Labels</li> <li>Customer Churn OpenScale Evaluation</li> <li>Customer Data Transactions</li> <li>Customer Personal Info Simplified</li> <li>Customer Training Data</li> </ul>"},{"location":"assets/#notebooks","title":"Notebooks","text":"<ul> <li>Churn Prediction Pyspark</li> <li>Move Between Deployment Spaces</li> </ul>"},{"location":"cicd/","title":"Governed MLOps CI/CD","text":""},{"location":"cicd/#introduction","title":"Introduction","text":"<p>As organizations scale adoption of AI models in production, it becomes more important to automate the process for testing, validating, and promoting such models from dev (development) to uat (user acceptance testing, also known as pre-prod, quality assurance or staging) to prd (production) environments.</p> <p>To enable such automation, it is important to be able to validate and monitor the performance of AI models (fairness, quality, drift, explainability) as well as automate the process of propagating the models and associated assets from one environment to another. A typical scenario for developing and promoting AI models includes the following sequence:</p> <ol> <li> <p>Data scientists explore multiple algorithms and techniques to train best performing AI model.</p> </li> <li> <p>Once satisfied with performance results, data science lead deploys the best performing model to a Pre-Prod deployment space.</p> </li> <li> <p>MLOps team configures Watson OpenScale to monitor and run validation tests against the model deployed in Pre-Prod space.</p> </li> <li> <p>Once model validation is approved, MLOps team propagates model from Pre-Prod to Prod.</p> </li> <li> <p>MLOps team configures Watson OpenScale to monitor the production model for fairness, quality and drift.</p> </li> </ol> <p>In this workshop, we cover two common approaches for implementing a governed MLOps methodology to enable the automation of propagating models from development through user acceptance testing (PreProd) to production:</p> <ol> <li> <p>Propagating data science assets including trained models from one environment to another.</p> </li> <li> <p>Git based automation where data assets and source code are checked into a git repository, git integration is leveraged for code management, and automation is leveraged for testing and validation in UAT (PreProd) and production environments.</p> </li> </ol> <p>Choosing which of these approaches to implement is mostly dictated by the use case and the preferred method of governance that an organization chooses to adopt. In this workshop, we highlight both approaches and how they can be implemented using Cloud Pak for Data.</p> <p>Before diving into the details of these approaches, it is helpful to quickly review the overall data science process and various tasks/activities performed by the data science team.</p> <p>Initially, data science team engages with business stakeholder to discuss the business problem to be addressed. After understanding and scoping the business problem, data scientists search and find data assets in the enterprise catalog that may be relevant and useful for training AI models to address the identified business problem.</p> <p>Data scientists experiment with various data visualizations and summarizations to get a sound understanding of available data. This is because real-world data is often noisy, incomplete, and may contain wrong or missing values. Using such data as-is can lead to poor models and wrong predictions.</p> <p>Once relevant data sets are identified, data scientists commonly apply \u201cFeature Engineering\u201d which is the task of defining and deriving new features from existing data features to train better-performing AI models. The feature engineering step includes aggregation and transformation of raw variables to create the features used in the analysis. Features in the original data may not have sufficient predictive influence and by deriving new features, data scientists train AI models that deliver better performance.</p> <p>Afterwards, data scientists train machine learning models using the cleansed data prepared in the previous steps. Data scientists train several machine learning models, evaluate them using a holdout data set (data not used at training time) and select the best model or multiple models (ensemble) to be deployed in the next phase. Model building usually also includes a hyperparameter optimization step, which aims at selecting the best set of model hyperparameters (i.e. parameters of the model itself), which are set before training starts to further increase model performance</p> <p>After data scientists build (train) an AI model that meets their performance criteria, they make that model available for other collaborators, such as software engineers, other data scientists, and business analysts, to validate (or quality test) the model before it gets deployed to production.</p> <p>Once a model has gone through the iterations of development, build, and test, the Machine Learning Operations (or MLOps) team deploys the model into production. Deployment is the process of configuring an analytic asset for integration with other applications or access by business users, to serve production workload at scale. Two most popular types of deployment are:</p> <ul> <li> <p>Online: a real time request/response deployment option. When this deployment option is used, models or functions are invoked with a REST API. A single row or multiple rows of data can be passed in with the REST request.</p> </li> <li> <p>Batch: a deployment option that reads and writes from/to a static data source. A batch deployment can be invoked with a REST API.</p> </li> </ul> <p>Keep in mind that in a hybrid multi-cloud world, development (dev), user-acceptance testing (uat), and production (prd) environment can be on-prem or in one of different cloud platforms. For example, the development environment can be hosted in a cloud platform but the production environment can be on-prem. Alternatively, the user acceptance testing environment can be on-prem while the production environment can be hosted on a public cloud platform.</p> <p>For some background on the model development and deployment in the AI Lifecycle Management process, please review the white paper or check the following blogs:</p> <ul> <li> <p>AI Model Lifecycle Management: Overview</p> </li> <li> <p>AI Model Lifecycle Management: Build</p> </li> <li> <p>AI Model Lifecycle Management: Deploy</p> </li> </ul> <p>Additionally, please review the Jenkins + IBM Cloud Pak for Data == Production-ready Delivery Pipelines for AI blog to learn how to leverage cpdctl and Jenkins to design pipelines to enable automation in propagating AI models to production.</p> <p>Lastly, the following lab in this workshop shows how to use Watson Studio Pipelines to simplify the overall MLOps process by better orchestration of the various stages required for developing robust AI models.</p>"},{"location":"cicd/#exercise-1-ai-model-propagation-across-environments","title":"Exercise 1 \u2013 AI Model Propagation across Environments","text":""},{"location":"cicd/#introduction_1","title":"Introduction","text":"<p>In this section, we illustrate the first approach leveraging cpdctl, a Cloud Pak for Data CLI (command line interface) tool, to automate the process of propagating trained models from one environment to another.</p> <p>In practice, the environments can exist in the same Cloud Pak for Data cluster or in completely different Cloud Pak for Data clusters hosted on different cloud platforms. For this lab, we will use the same Cloud Pak for Data cluster and illustrate how to use the cpdctl tool to propagate assets from the quality assurance (QA, also known as user acceptance testing) deployment space to the production deployment space. The process is identical to how you\u2019d propagate models from one cluster to another as the cpdctl tool is designed to handle the hybrid multi-cloud seamlessly.</p>"},{"location":"cicd/#using-cpdctl-in-a-notebook","title":"Using cpdctl in a Notebook","text":"<p>You will run a notebook to leverage cpdctl to copy assets from churnUATspace to churn_prod_space.</p> <ol> <li> <p>Navigate to your Customer Churn Prediction project.</p> </li> <li> <p>On the project page, click the Assets tab, then New Asset and select Jupyter notebook editor.</p> <p></p> <p></p> </li> <li> <p>On the New notebook page, click the Local file tab and then browse to find the notebook file CopyAssets_DeploymentSpace1_to_DeploymentSpace2.ipynb that you downloaded at the beginning of the workshop. Then click Create.</p> <p></p> <p>IMPORTANT</p> <p>DO NOT RUN THE NOTEBOOK YET</p> </li> <li> <p>The notebook will load in edit mode. DO NOT RUN THE NOTEBOOK YET, you must change some variable values to customize the environment URL and your Model Name. Go to the cell shown below:</p> <p></p> <p>You need to set the values of these variables by changing the values in red. Use the following values and keep the single quotes:</p> <ul> <li> <p>SOURCE_CPD_USERNAME: Your workshop username</p> </li> <li> <p>SOURCE_CPD_PASSWORD: Your workshop password</p> </li> <li> <p>SOURCE_CPD_URL: The URL that you use to access the CP4D workshop environment. The following is just an example:</p> <p>https://cpd-cpd.apps.ocp-663004oi6y-lgmy.cloud.techzone.ibm.com</p> </li> <li> <p>TARGET_CPD_USERNAME: Your workshop username</p> </li> <li> <p>TARGET_CPD_PASSWORD: Your workshop password</p> </li> <li> <p>TARGET_CPD_URL: In this case, you're going to promote models between deployment spaces in the same environment. Therefore, use the same value as in the previous SOURCE_CPD_URL</p> </li> <li> <p>SOURCE_DEPLOYMENT_SPACE_NAME: churnUATspace</p> </li> <li> <p>TARGET_DEPLOYMENT_SPACE_NAME: churn_prod_space</p> </li> <li> <p>TARGET_MODEL_NAME: &lt;YourUser&gt;_ChurnPredictionProd</p> </li> </ul> <p>This is an example for user0:</p> <p></p> </li> <li> <p>Now run all the steps in the notebook. No error should be shown.</p> </li> </ol>"},{"location":"cicd/#checking-the-production-model","title":"Checking the Production model","text":"<ol> <li> <p>Go to the Deployments menu to check that the model has been promoted to Production.</p> <p></p> </li> <li> <p>On the Deployments page, click the Spaces tab and select the churn_prod_space.</p> <p></p> </li> <li> <p>Go to the Deployments tab and click on the production deployment &lt;YourUser&gt;_ChurnPredictionProd.</p> <p></p> </li> <li> <p>Click the Test tab. You're going to test the deployed model with some data. Go to the section JSON and introduce the following data:</p> <p>Warning</p> <p>Use the Copy button in the following code section to keep the right format when pasting it.</p> <p>Tip</p> <p>Feel free to edit/change values and re-run the prediction to see how different features can impact the prediction.</p> <pre><code>{\n\"input_data\": [\n{\n\"fields\":[\"ID\",\"LONGDISTANCE\",\"INTERNATIONAL\",\"LOCAL\",\"DROPPED\",    \"PAYMETHOD\",\"LOCALBILLTYPE\",\"LONGDISTANCEBILLTYPE\",\"USAGE\",\"RATEPLAN\",  \"GENDER\",\"STATUS\",\"CHILDREN\",\"ESTINCOME\",\"CAROWNER\",\"AGE\"],\n\"values\":[[1,28,0,60,0,\"Auto\",\"FreeLocal\",\"Standard\",89,4,\"F\",\"M\",1,    23000,\"N\",45]]\n}\n]\n}\n</code></pre> <p>Then click Predict.</p> <p></p> </li> <li> <p>The prediction for this input data is shown.</p> <p></p> </li> </ol> <p>Info</p> <p>At this point you\u2019ve seen how to run a notebook leveraging cpdctl tool to propagate assets from one deployment space to another. You can create and schedule a job to run periodically and execute this sample notebook.</p>"},{"location":"cicd/#exercise-2-git-based-flow","title":"Exercise 2 \u2013 Git-based Flow","text":""},{"location":"cicd/#introduction_2","title":"Introduction","text":"<p>In this part of the workshop, we are going to use Git based automation where data assets and source code are checked into a git repository for source code management, and automation is leveraged for testing and validation in UAT and production environments. Effectively we are propagating source code only and training and deploying models in each of the environments based on the notebooks.</p> <p>Git-based deployment flow: </p> <p>The figure above illustrates a typical deployment process of AI models following a governed MLOps methodology applied through Git integration. Typically, the enterprise would have separate clusters or namespaces (depending on isolation needs) to support the various stages of development (training / developing), validation (evaluation/pre-production) and deploying AI (production) models. Figure 1 depicts three git branches: dev (development), uat (user-acceptance testing), and prd (production) that correspond to the development, pre-production, and production clusters.</p> <p>Data scientists mainly operate in the development cluster and interact with the git dev branch. Data scientists leverage the development cluster for experimentation and exploration where they collaborate with other data scientists, data engineers, and business SMEs to identify the right data sets and train the best performing AI models.</p> <p>As denoted in the figure, there are typically multiple forks off the dev git branch to add features for improving the AI model. Once satisfied with the AI model that delivers best performance, data scientists check the code and assets into the git dev branch via pull requests. The data science project lead, who owns the dev git branch, approves the submitted pull requests and tests the notebook and model.</p> <p>After review (and there could be a few back-and-forth interactions), the lead data scientist would create a pull request (sometimes also referred to as merge request) to propagate the assets (notebooks) to the uat git branch for testing in the UAT environment, which typically references different data stores than the DEV environment.</p> <p>Deployment of the assets in the UAT environment (from the uat branch) is typically done via automation, also known as GitOps. A general policy in many organizations mandates that deployment of applications, which includes data science assets, is always fully automated without human intervention. First, this helps to streamline the processes but moreover, it reduces the risk of failing installations because the exact same process is executed in multiple stages before it reaches production.</p> <p>Automation would pull the assets from the uat git branch into the pre-production cluster to retrain the AI model and run validation testing against such model. The data used for validation is different from the data used for training and initial testing of the AI model. Validation is executed on the data assets in the uat branch.</p> <p>Once UAT validation tests are concluded, the final assets (code and data assets) are checked into production git branch via a pull request. The MLOps team lead reviews and approves the pull request to propagate the assets into the production git branch. Automation would pick up the assets from the production git branch and push those into the production cluster where the code is run to re-train the AI model and validate the performance. Assuming all performance checks meet expected targets, the model is deployed in the production cluster and is ready for inferencing at scale.</p>"},{"location":"cicd/#preparing-the-git-environment","title":"Preparing the Git environment","text":"<p>Info</p> <p>We have already prepared a Git repository that consists of 3 branches: prd (production), uat (UAT) and dev (development):     https://github.com/CP4DModelOps/mlops-churn-prediction.</p> <p>In this exercise you will create a fork of this repository to your own GitHub organization which you will create.</p> <ol> <li> <p>Go to https://github.com and login. If you don't have a Github account, you can create one for free.</p> </li> <li> <p>Click on your user at the right top of the screen and go to Settings</p> <p></p> </li> <li> <p>Click Organizations in the left-side menu, then New organization.</p> <p></p> </li> <li> <p>Click Create a free organization.</p> <p></p> </li> <li> <p>Enter the Organization account name, for example     \u201cMLOps-&lt;your_full_name&gt;\u201d. The name must be unique so you may     have to be creative. Also enter your e-mail address. Select     My personal account, Verify that you're human, accept the terms,     then click Next</p> <p></p> </li> <li> <p>On the next screen, just click \u201cComplete Setup\u201d and then \u201cSubmit\u201d</p> <p></p> <p>Info</p> <p>Now you have a new github.com organization which you can use to fork the training repository.</p> </li> <li> <p>Go to https://github.com/CP4DModelOps/mlops-churn-prediction and click the Fork button at the right top of the page.  </p> <p></p> </li> <li> <p>Select the organization you just created from the list and uncheck the option \"Copy the prd branch only\". Then click Create fork</p> <p></p> </li> <li> <p>GitHub will fork the repository and then show it. Click the area that says 3 branches and check that you have 3 branches: prd (default), uat and dev.</p> <p></p> <p></p> <p>Info</p> <p>We will now set up a branch protection rule to simulate a \u201creal\u201d scenario where pull requests have to be approved before being merged.</p> </li> <li> <p>Click on Settings and then Branches on the left-hand side.     You will see that there are no branch protection rules have been set     up yet. To protect the uat and prd branches we will create these     rules. Click Add branch protection rule </p> <p></p> </li> <li> <p>Create a new rule for the uat branch. Enter uat for the Branch name pattern and select the Require a pull request before merging checkbox.</p> <p></p> <p>also scroll down and select the Restrict who can push to matching branches checkbox. Then click Create:</p> <p></p> </li> <li> <p>Repeat the above steps for the prd branch.</p> </li> <li> <p>When done you should see the following branch protection rules:</p> <p></p> <p>Info</p> <p>As your organization only has a single user (yourself) and you are the administrator, we cannot truly set up a formal approval process in which data scientists only have write access to the repository and the MLOps staff can approve requests. In an actual implementation, the production branch could also reside in an upstream repository with different permissions. Setting up the GitHub topology with different repositories and teams is beyond the scope of this workshop.</p> </li> </ol>"},{"location":"cicd/#watson-studio-integration","title":"Watson Studio integration","text":"<p>Now that you have finished creating your repository, we can start integrating it with Watson Studio in Cloud Pak for Data. Watson Studio needs to be given access to your repository via a token.</p> <ol> <li> <p>Click on your user at the right top of the page and select    Settings.  </p> <p></p> </li> <li> <p>Scroll down until you see Developer settings in the left sidebar and click on it.</p> <p></p> </li> <li> <p>Click on Personal access tokens &gt; Tokens (classic) then Generate new token &gt; Generate new token (classic).</p> <p></p> </li> <li> <p>Enter the name of the token and ensure the repo box is checked. Then scroll down and click Generate token.</p> <p> </p> </li> <li> <p>The token is displayed only once; make sure you copy it. You will need it multiple times during the following steps.</p> <p></p> <p>Info</p> <p>In IBM Cloud Pak for data, a project is how you organize your resources to achieve a particular goal. A project allows for high-level isolation, enabling users to package their project assets independently for different use cases or departments. Your project resources can include data, collaborators, scripts, and analytic assets like notebooks and models.</p> </li> </ol>"},{"location":"cicd/#creating-a-git-enabled-project","title":"Creating a Git-enabled project","text":"<ol> <li> <p>Go to All projects by clicking on the Navigation menu and selecting Projects &gt; All projects.</p> <p></p> </li> <li> <p>Click on New project to create a new project.</p> <p></p> </li> <li> <p>Select the Create a project integrated with a Git repository option.</p> <p></p> </li> <li> <p>On the New project page, provide the Name &lt;YourUser&gt;-mlops-dev and. For integration type, select the Default Git integration. You will need to upload the GitHub token you generated before token to connect to your Git repository. Click the New Token link.</p> <p></p> </li> <li> <p>On the Git integration pop-up, select Github, provide your git access token and give it a name for reference: &lt;YourUser&gt;-mlopstoken. Click Continue.</p> <p></p> </li> <li> <p>Next click the token drop down and select the token you created &lt;YourUser&gt;-mlopstoken.</p> <p></p> </li> <li> <p>Next step is to provide the GitHub repository and branch to associate with your project.</p> <p>Warning</p> <p>The rest of the instructions in this module reference the following repository: https://github.com/MLOps-DataScience/mlops-churn-prediction but you will reference your own repository you forked from the upstream repository.</p> <p>For the Repository URL field, provide HTTPS reference for your Git repository. You can copy this by clicking on the Code button in the Code tab, then then the Copy button indicated by the red arrow below.</p> <p></p> </li> <li> <p>Go back to your Create project window and paste the URL in the Repository URL field. Then, set teh Branch field to dev. Click Create.</p> <p></p> </li> <li> <p>You should see a pop-up message stating that the project is created successfully. Click on the View new project link to navigate to the new project.</p> <p>Warning</p> <p>If the completion windows does not show up after about 5 minutes, refresh the page and check if the project was cloned successfully.</p> <p></p> </li> <li> <p>Take a few minutes to review the project information. The Overview tab provides general information about the project like Date created, the Collaborations, and Storage used. Click the Manage tab.</p> <p></p> </li> <li> <p>Click Access Control option to review which users have access to the platform. In this case, you're the only collaborator in this project.</p> <p></p> <p>Tip</p> <p>You can add other users or user groups and assign them the relevant permissions to collaborate on this project. Typically, a data science project involves multiple users and roles who collaborate on developing and evaluating AI models.    </p> </li> <li> <p>Click the Assets tab to review the assets associated with the project. Only the CUSTOMER_DATA_ready.csv file is shown as an asset. Notebooks and other data assets can be found from the JupyterLab interface.  </p> <p></p> </li> <li> <p>Now you can start working with JupyterLab IDE in this Git-integrated project. Click on the Launch IDE link at the top of the page and select JupyterLab.</p> <p></p> </li> <li> <p>On the panel that shows up, select JupyterLab with IBM Runtime 23.1 on Python 3.10 and click Launch.</p> <p></p> </li> <li> <p>Once JupyterLab has started, take some time to look at the different areas of the workpace:</p> <ul> <li> <p>On the far-left side (marked with a red box) you can browse the folder contents, view the active kernels and other tabs and view the Git repository state such as current branch and any changes that have already been done.</p> </li> <li> <p>Also on the left-hand side (annotated with a blue box) you see the \u201croot\u201d folder of the repository with the assets folder. This is the folder where the input files and notebooks are held.  </p> </li> <li> <p>The middle part (marked by an amber box) is used to create new notebooks, open an Python console, a shell terminal and a few other applications.</p> </li> </ul> <p></p> </li> <li> <p>In this exercise, you are going to make a change to an existing notebook. First, make sure we make the changes in a \u201cfeature\u201d branch so that can validate the changes without changing the notebook that may be used by other team members.</p> <p>Click on the Git icon at the far left as indicated by the red arrow.</p> <p></p> <p>Info</p> <p>You will see multiple branches in the current Git project (dev, origin/HEAD, origin/dev, origin/uat and origin/prd) and your current branch is dev.</p> </li> <li> <p>Click Current branch to show all branch options, then create a new branch by clicking the New Branch button.</p> <p></p> </li> <li> <p>A new window appears in which you can enter the name of the new branch. Use optimize-churn-model name. Then click Create Branch.</p> <p></p> </li> <li> <p>Click the folder icon, then vavigate to the folder assets/notebooks. Then, open the customer-churn-prediction.ipynb notebook by double-clicking it.</p> <p> </p> </li> <li> <p>Scroll down to the section above Evaluate and notice the accuracy of the model. This is currently approximately 0.92.</p> <p></p> </li> <li> <p>Scroll up to where the data is split and change the test_size from 0.4 to 0.2.</p> <p></p> </li> <li> <p>Save the notebook by clicking the disk at the top left of the notebook.</p> <p></p> <p>Info</p> <p>In a real scenario, the Data Scientist would run the notebook to check the accuracy of the model and validate that the change was positive.</p> </li> <li> <p>Assume that you\u2019ve run the notebook and you're happy with the change results and you wish to promote the model to the dev environment. Open the Git panel.</p> <p>Note that the notebook you changed appears in the list of \u201cChanged\u201d files:</p> <p></p> </li> <li> <p>Click on the notebook and then on the \u201c+\u201d sign to stage the change for commit.</p> <p></p> <p>The notebook will now appear in the list of \u201cStaged\u201d files.</p> </li> <li> <p>Enter a meaningful message in the \u201cSummary\u201d field and click the Commit button.</p> <p></p> </li> <li> <p>The change is still only local to JupyterLab. To make it available in the Git repository, you need to synchronize. Click on the cloud button to push the changes to the remote repository.</p> <p></p> </li> <li> <p>You can now go to your repository on GitHub, refresh the page and     select the new branch that you created. Then, click 1 commit ahead.</p> <p></p> </li> <li> <p>As you want the data science lead to review your changes, create a pull request. Please pay attention to ensure you are creating a pull request for your own repository. You will find that GitHub assumes you want to create a pull request to the upstream repository (CP4DModelOps/mlops-churn-prediction). Set the base repository box to your own repository.</p> <p></p> <p>Select the dev branch of your own.</p> <p></p> </li> <li> <p>GitHub indicates that it is able to merge the changes automatically(Able to merge). Click the Create pull request button to request the changes to be merged into the dev branch.</p> <p></p> <p>Then click Create pull request again.</p> <p></p> <p>Info</p> <p>In a real-case scenario, the Data Scientist lead would receive an e-mail informing that a pull request is ready to be reviewed. The next steps would be carried out by them.</p> </li> <li> <p>Let's approve and merge the pull request. Click on the Pull requests link at the top. You will see a list of all pull requests Click the pull request to open it.</p> <p></p> </li> <li> <p>See that the request consists of 1 commit and only 1 file was changed. Click on the Files changed box to see the details of what was changed.</p> <p></p> <p>The file is not nicely formatted as a notebook but it is quite easy to identify the changes that have been made. If wanted you can click on the ellipses indicated by the red oval and select View file to display the file in a notebook format.</p> <p></p> <p>Info</p> <p>The data scientist lead is happy with the changes and will now approve and merge the changes to the dev branch. As we only have one user for this repository, we cannot approve our own pull request. Normally, the lead data scientist would use the Review changes button to approve the request.</p> </li> <li> <p>Click your browser\u2019s \u201cback\u201d button to go to the pull request.</p> </li> <li> <p>Go back to the Pull Request. Then click the Merge pull request button to merge the changes with the dev branch.</p> <p></p> <p>Then, click Confirm merge.</p> <p></p> <p>GitHub confirms that the pull request has been successfully merged and closed. As a good practice to keep the repository clean, delete the branch by clicking the Delete branch button.  </p> <p></p> </li> <li> <p>Now the dev branch has been updated with the change. This is how MLOps works with the git integration. Next steps would be to promote the change to the UAT and Production branches, and to deploy the model to each correspondent Deployment Space.</p> </li> <li> <p>Go to your Cloud Pak for Data window. Log off as the datascientist user and log in as the data scientist lead (dslead user) and     navigate to the mlopsdev project. Open it and start JupyterLab     like you did before.</p> </li> </ol> <p>Info</p> <p>You've reached the end of this lab.</p>"},{"location":"demo-assistant/","title":"Churn Aware Assistant","text":"<p>Warning</p> <p>This lab is pending an update, as deprecated features are being used. However, it can give you an idea on how the model can be consumed in a real use case.</p> <p>This lab highlights one scenario to illustrate how AI models can be infused into an application to improve customer interaction. Specifically, you will learn to develop a solution for Personalizing Customer Experience by infusing the churn prediction AI model in your Virtual Assistant. In the previous sections of the workshop, you have learned how to leverage Cloud Pak for Data to execute the following tasks:</p> <ol> <li> <p>Access data from different sources.</p> </li> <li> <p>Catalog and apply enterprise governance to that data.</p> </li> <li> <p>Train AI models using Jupyter notebooks and AutoAI capabilities.</p> </li> <li> <p>Deploy trained AI models to preProd (or QA) deployment spaces for validation.</p> </li> <li> <p>Validate AI model for fairness, quality, and drift using Watson OpenScale.</p> </li> <li> <p>Once model validation is approved, automate CI/CD process to push models from UAT (Pre-Prod) deployment spaces to Prod deployment spaces.</p> </li> <li> <p>Monitor AI models in production using Watson OpenScale for accuracy, drift, bias, and explainability to deliver trusted AI.</p> </li> </ol> <p>Info</p> <p>As a quick reminder, the business use case involves improving the customer experience by offering a personalized interaction from the virtual agent based on the likelihood of a customer to churn.</p>"},{"location":"demo-assistant/#pre-requisites","title":"Pre-Requisites","text":"<p>Watson Assistant is offered as a cartridge on Cloud Pak for Data as well as it is offered as a managed service on IBM Cloud. The lab teaches how to infuse the trained AI model for churn prediction in a Virtual Assistant designed with Watson Assistant on IBM Public Cloud.</p> <p>To complete the lab, you will need to setup and access an IBM Public Cloud account.</p>"},{"location":"demo-assistant/#cloud-functions-wrapper","title":"Cloud Functions Wrapper","text":"<p>In this section, we will create an IBM Cloud Functions wrapper to the model we trained and deployed in the previous sections. IBM Cloud Functions is IBM\u2019s Function as a Service (FaaS) cloud computing service that allows you to execute code in response to events without dealing with the complexity of setting up the required infrastructure.</p> <ol> <li> <p>Log into your IBM Cloud account by navigating to https://cloud.ibm.com and providing your log-in credentials. If     you don\u2019t have an IBM Cloud account, use the links on the landing page to create a free IBM Cloud account.</p> </li> <li> <p>Logged into your IBM Cloud account, navigate to the IBM Cloud Menu and select Functions.</p> <p></p> </li> <li> <p>On the IBM Cloud Functions page, click on Start Creating.</p> <p></p> </li> <li> <p>Click Create Action. Alternatively, you can also click Actions in the left navigation column and then click Create on the Action page that loads.</p> <p></p> </li> <li> <p>On the Create Action page, provide a name for your Cloud Functions action, select \u201cPython 3.7\u201d for the Runtime and click Create.</p> <p>Tip</p> <p>Note that you can create the action using a different Runtime but in this lab we\u2019re using Python 3.</p> <p></p> </li> <li> <p>Replace the contents of the Code section of the action with the code shown below.</p> <p>IMPORTANT</p> <p>Use the copy button in the code snippet below to keep the right format of the content when pasting.</p> <pre><code>#\n#\n# main() will be run when you invoke this action\n#\n# @param Cloud Functions actions accept a single parameter, which must be a JSON object.\n#\n# @return The output of this action, which must be a JSON object.\n#\n#\nimport sys\nimport requests, json\nimport ast\n\ndef getToken(cpd_url,cpd_username,cpd_password):\n    url = cpd_url + '/v1/preauth/validateAuth'\n    #print(\"get token url: \", url)\n    response = requests.get(url,auth=(cpd_username,cpd_password),verify=False)\n    #print(\"response: \", response)\n    wml_token = response.json()[\"accessToken\"]\n    #print(\"token: \", wml_token)\n    return wml_token\n\ndef predictChurn(wml_token,scoring_url,flds,vals):\n    header = {'Content-Type': 'application/json', 'Authorization': 'Bearer ' + wml_token}\n\n    payload_scoring = {\"input_data\": [{\"fields\": flds, \"values\": vals}]}\n    response_scoring = requests.post(scoring_url, json=payload_scoring, headers=header, verify=False)\n    churn = json.loads(response_scoring.text)\n    return churn\n\ndef getProb(prediction):\n    pred = prediction['predictions'][0]['values'][0][-1]\n    if pred == 'T':\n        prob = prediction['predictions'][0]['values'][0][1][1]\n    else:\n        prob = prediction['predictions'][0]['values'][0][1][0]\n    return prob\n\ndef main(dict):\n    cpd_url = dict['cpd_url']\n    cpd_username = dict['cpd_username']\n    cpd_password = dict['cpd_password']\n    try:\n        scoring_url = dict['scoring_url']\n        flds_array = ast.literal_eval(dict['fields'])\n        vals_array = ast.literal_eval(dict['values'])\n    except:\n        print(\"error\")\n        response = {\"error\": \"not all required parameters are provided. Please make sure you pass the scoring_url, fields, and  values parameters\"}\n        return response\n\n    wml_token = getToken(cpd_url,cpd_username,cpd_password)\n    churn_prediction = predictChurn(wml_token,scoring_url,flds_array,vals_array)\n    #print(\"churn prediction: \", churn_prediction)\n    predlabel = churn_prediction['predictions'][0]['values'][0][0]\n    print(\"predlabel: \", predlabel)\n    predprob = getProb(churn_prediction)\n    print(\"predprob: \", predprob)\n    response = {\"label\": predlabel, \"prob\":predprob}\n    return response\n</code></pre> <p>After you paste the code, click Save.</p> <p></p> <p>Info</p> <p>Feel free to review the simple code to understand what it does. It gets a token from IBM Cloud Pak for Data using the username and password credentials that you provided and then makes a call to the deployed machine learning model using that token to predict churn.</p> </li> <li> <p>Next, click on Parameters in the left column and click Add Parameter to provide required parameters to pass to the code. Specifically, this is where you can specify authentication information like Cloud Pak for Data URL, Cloud Pak for Data username, and Cloud Pak for Data password. For this action, we need three parameters:</p> <ul> <li>cpd_url</li> <li>cpd_username</li> <li>cpd_password</li> </ul> <p>Info</p> <p>You should have these values from the previous sections in this workshop. The cpd_url is the url you used to access Cloud Pak for Data landing page. The cpd_username and cpd_password are the username and password you used to log into Cloud Pak for Data.</p> <p>IMPORTANT</p> <p>Please make sure to include double quotes \"\" when providing the values for the parameters.</p> <p>Click Save after providing the parameters. </p> <p></p> </li> <li> <p>Next, click back on Code (in the left navigation column) and test the action by clicking on Invoke with Parameters and provide the following values in the Change Action Input window. If you don't see the Invoke with Parameters option, press Save.</p> <ul> <li> <p>scoring_url This would be the REST endpoint from the deployed machine learning model you trained in the previous section. If you don\u2019t have the scoring_url for your deployed model, you can get it by logging into your Cloud Pak for Data instance, navigating to Menu Deployments, selecting the deployment space, and clicking on the specific deployment. On that page, you copy the Endpoint value as that will be your scoring_url.</p> </li> <li> <p>fields The array of fields used needed for scoring the model to obtain likelihood of a user to churn.</p> </li> <li> <p>values The array of values corresponding to the field</p> </li> </ul> <p></p> <p>The parameters you pass to your cloud function have the following format:</p> <pre><code>{\n\"scoring_url\":\"&lt;YOUR_DEPLOYED_WML_MODEL_SCORING_ENDPOINT&gt;\",\n\"fields\":\"['ID', 'LONGDISTANCE', 'INTERNATIONAL', 'LOCAL', 'DROPPED', 'PAYMETHOD','LOCALBILLTYPE', 'LONGDISTANCEBILLTYPE',  'USAGE', 'RATEPLAN', 'GENDER', 'STATUS','CHILDREN', 'ESTINCOME', 'CAROWNER', 'AGE']\",\n\"values\":\"[[1,27,6,33,1,'CC','Budget','Standard',400,3,'M','S',3,46000,'Y',38]]\"\n}\n</code></pre> <p>Click Apply.</p> <p></p> </li> <li> <p>Click Invoke and the action will run and returns the results which is the prediction and associated probabilities for the predicted value. Please note that the acutal value you get back may be different depending on what input values you provided.</p> <p></p> </li> <li> <p>Next enable your cloud functions action as a web action to handles HTTP events. Click on Endpoints in the left column (annotated with red rectangle in figure below), then check the Enable as Web Action checkbox (annotated with red arrow in figure below) and click Save (annotated with red oval in figure below). Copy the URL (annotated with red arrow in figure below) as that will be needed to call this web action in subsequent steps of this lab. The url will have the following form:</p> <p>https://eu-de.functions.cloud.ibm.com/api/v1/web/&lt;your_ibm_id&gt;_&lt;space&gt;/default/&lt;action_name&gt;</p> <p></p> </li> </ol>"},{"location":"demo-assistant/#watson-assistant-setup","title":"Watson Assistant Setup","text":"<p>In this section, we will use Watson Assistant service to create an assistant and associate it with a dialog skill. We will integrate the dialog skill with the Cloud Functions action via the Assistant\u2019s webhook capability to personalize the user experience and respond to each user differently depending on the likelihood of that user churning.</p> <ol> <li> <p>Logged into your IBM Cloud account, navigate to your Watson Assistant service and launch it. If you don\u2019t have a Watson Assistant service, click on Catalog (annotated with red oval in figure below) and select AI / Machine Learning from left navigation column (annotated with red rectangle in figure below) and then select Watson Assistant. Alternatively, you can type \u2018Watson Assistant\u2019 in the search bar (annotated with red arrow) and then select the Watson Asssistant services under Catalog Results.</p> <p></p> </li> <li> <p>On the Watson Assistant page, select the Trial plan and click Create. You can optionally provide a name for your Watson Assistant service. Please note you can only have one Watson Assistant service with the Trial (or Lite) plan so if you have created one already please use that one. Once the service is created, click Launch Watson Assistant button.</p> </li> <li> <p>On the IBM Watson Assistant wizard, provide a name for your assistant and a description (optional) and click Next.</p> <p></p> </li> <li> <p>Fill the form with the following information, then click Next.</p> <ul> <li>Web</li> <li>N/A</li> <li>Developer</li> <li>I want to provide confident answers to common questions</li> </ul> <p></p> </li> <li> <p>Choose your favorite accent color then click Next.</p> <p></p> </li> <li> <p>Click Create</p> <p></p> <p>Info</p> <p>An assistant can handle Actions/dialog and/or search skills.</p> <ul> <li> <p>Actions/dialog skill uses natural language processing and machine learning technologies to understand user requests and respond appropriately based on specific responses you\u2019ve created in the dialog. A dialog skill is ideal for handling FAQ-style requests. The Actions skill lets you have an assistant ready to chat quickly by composing step-by-step flows for a range of simple to complex conversations.</p> </li> <li> <p>A search skill helps in answering complex (long-tail) questions by finding relevant information in enterprise data sources. It leverages Watson Discovery for collecting, enriching and indexing the enterprise data sources.</p> </li> </ul> </li> <li> <p>In this lab, we will work with the Dialog skill. In order to activate it, go to the Settings tab in the left-side menu and click Activate dialog</p> <p></p> </li> <li> <p>Next, we\u2019ll add a new intent: #activate_device. The scenario we\u2019re addressing is that of a user interacting with the chatbot to request activation of a new phone. Then depending on the churn likelihood of that user, the dialog will reflect different interactions. For users with high churn probability, the chatbot will connect them directly to an agent for a better experience. On the other hand, users with low churn probability, the chatbot will step them through the activation process directly. Note that for your use case, you can include other factors in deciding how to offer best experience for a user but in this lab, we would like to illustrate how to integrate the assistant with a trained machine learning model to offer a more personalized user experience.</p> <p>Click on Intents in the left navigation column and click Create intent.</p> <p></p> <p>Tip</p> <p>Note that you can also import intents directly from a csv file as explained in the Creating intents section in the documentation.</p> </li> <li> <p>Provide a name for your intent, #activate_device, and click Create intent.</p> <p></p> </li> <li> <p>Next, we need to provide some sample utterances that indicate this new intent. Add the following examples one by one under the User example field:</p> <ul> <li>can you help me activate my new device</li> <li>how can I activate my new phone</li> <li>I bought a new phone and would like to make it active with my number</li> <li>I would like to activate my device</li> <li>what do I need to do to map my number to my new phone</li> </ul> <p>Tip</p> <p>Note that as you add more examples, Watson Assistant starts training. You can view that by clicking the Try it button.</p> <p></p> </li> <li> <p>Next we need to update the dialog flow to handle device activation requests. Click on the back arrow (annotated with red rectangle in figure above; to the left of the intent name), then select Dialog in the left navigation column (annotated with red rectangle in figure below). Click the Menu on the \"Welcome\" node (three vertical dots annotated with red oval in figure below) and select Add node below (annotated with red arrow in figure below).</p> <p></p> </li> <li> <p>On the node window, provide a name for the node (for example, \u201cactivate device\u201d, annotated with red arrow in figure below), specify the condition under \u201cIf assistant recognizes\u201d as #activate_device intent and add a response \u201cHappy to help you with that\u201d. Once you\u2019ve added these, click the Try it button (if not open already) and test it by typing \u201cI would like to activate my device\u201d. Note the response from the Assistant \u201chappy to help you with that\u201d.</p> <p></p> </li> <li> <p>Next, create a new node following the same procedure to transfer the client to an agent. For demo purposes, we'd just simulate that the client is being transferred. In a real-life scenario, we would call the integration with the Call Center to call an agent.</p> <p></p> </li> <li> <p>Let's enable webhooks and connect the dialog node we just created to the web action we created in the previous section using IBM Cloud Functions. Click on Options in the left navigation column and then select Webhooks (annotated with red oval). In the URL field, provide the url you copied from the action Endpoint in IBM Cloud Functions (annotated with red rectangle in figure below). It will have the following:</p> <p>https://es-de.functions.cloud.ibm.com/api/v1/web/&lt;your_ibm_id&gt;_&lt;space&gt;/default/&lt;action_name&gt;</p> <p>IMPORTANT</p> <p>Note that you should append a .json to the end of that url or else you will get an error in assistant. For example, the url could be:</p> <p>https://es-de.functions.appdomain.cloud/api/v1/web/deb04933-f915-4141-a54b-ae5816c288a0/default/cpd_predictChurn.json</p> <p></p> </li> <li> <p>After adding the URL in Webhook setup page, click back on the Dialog tab in left navigation column (annotated with red arrow in figure below) then select the \u201cactivate device\u201d node and click Customize.</p> <p></p> </li> <li> <p>On the Customize \u201cactivate device\u201d page, turn Webhooks on by moving the slider from Off to On and click Apply.</p> <p>Tip</p> <p>You can safely ignore the warning about the webhook URL not being configured.</p> <p></p> </li> <li> <p>Once the Webhooks are turned On for this node, the \u201cactivate device\u201d node gets updated to reflect the Parameters that can be provided to the webhook url. If you remember in the previous section, the cpd_predictChurn action required the following 3 parameters:</p> <ul> <li>scoring_url</li> <li>fields</li> <li>values</li> </ul> <p>Specify these parameters in the \u201cactivate device\u201d node. The scoring_url is the endpoint for the deployed machine learning model. The fields is the list of features to pass to the model for predicting likelihood of churn. The values is the array of values for the fields specific to the user.</p> <p>IMPORTANT</p> <p>Remember to include double quotes \"\" for the parameter values.</p> <ul> <li>scoring_url: \"#########\"</li> <li>fields:\"['ID', 'LONGDISTANCE', 'INTERNATIONAL', 'LOCAL', 'DROPPED', 'PAYMETHOD', 'LOCALBILLTYPE', 'LONGDISTANCEBILLTYPE', 'USAGE', 'RATEPLAN', 'GENDER', 'STATUS', 'CHILDREN', 'ESTINCOME', 'CAROWNER', 'AGE']\"</li> <li>values: $user_vals</li> </ul> <p>The figure below shows how the parameters should be defined (annotated with red rectangles). To add new parameters, you click the Add parameter (annotated with red oval in figure below). Note that the values parameter is defined as a context variable which would be different for different end users. The actual field values can be obtained from a backend system when a user authenticates into the system and initiates the chatbot interaction. For purposes of this lab, we emulate this behavior by setting the context variable ($user_vals) directly using the \u201cManage Context\u201d functionality in the \u201cTry it\u201d panel. In a production environment, this context variable would be set by a back end system once the user is authenticated.</p> <p></p> <p>Info</p> <p>Now that we\u2019ve integrated the webhook in the \u201cactivate device\u201d node, we edit the response to handle the results from the cloud function indicating likelihood of user to churn. If the prediction likelihood returns False (\u2018F\u2019), the virtual assistant will help the user with device activation process. If the prediction likelihood return True (\u2018T\u2019), on the other hand, the virtual assistant will transfer the user to a human agent to guarantee best experience for the user.</p> </li> <li> <p>Scroll down in the \u201cactivate device\u201d node to the \u201cAssistant responds\u201d section and add the following condition and response:</p> <p>IF ASSISTANT RECOGNIZES =&gt; $webhook_result_1 &amp;&amp; $webhook_result_1.label == \"F\"</p> <p>RESPOND WITH =&gt; I would be happy to help you with activating your device</p> <p>You can either enter the condition and response values directly in this window or you can click the Customize response for a better view.</p> <p></p> <p>If you click Customize response (gear icon next to the condition/response), you will see the following figure. The first condition (annotated with red oval) indicates that we have a valid context variable defined which means the webhook returned a valid response and save the value in $webhook_result_1. The second condition (annotated with red rectangle) indicates that the response from the churn prediction model is \u201cF\u201d (false); $webhook_result_1.label == \u201cF\u201d. For these conditions, the virtual assistant will respond with \u201cHappy to help you with that request\u201d. Click Save to save that dialog response.</p> <p>This effectively checks if the returned prediction is false (no churn) and if so, responds to assist the user with device activation.</p> <p></p> </li> <li> <p>Back on the activate device node, click Add response and add the following condition.</p> <p></p> <p>IF ASSISTANT RECOGNIZES =&gt; $webhook_result_1 &amp;&amp; $webhook_result_1.label == \"T\"</p> <p>RESPOND WITH =&gt; Thank you for being a great customer</p> <p></p> </li> <li> <p>Click on the arrow to move this response to be the second (above the anything_else response). anything_else response should always be the last condition.</p> </li> <li> <p>Click the Customize response (gear icon annotated with red arrow in figure above) to customize the response for the scenario when the churn prediction is true. In the \u201cConfigure response 2\u201d window, click on Default to node settings (annotated with red arrow in figure below) and then select Jump to (annotated with red oval in figure below). This will take you back to the overall dialog flow so you can select which node to \u201cJump to\u201d.</p> </li> <li> <p>Select the \u201cPlease transfer me to an agent\u201d node (annotated with red rectangle in figure below) and click Respond. Click Save to close the Configure Response window.</p> <p></p> <p></p> </li> <li> <p>Next, we test the dialog. To emulate different users, click the Try it button (if the Try it out panel is not open already) and click on Manage Context (annotated with red arrow in figure below). Then add a context variable and call it user_vals (annotated with red oval in figure below) and hit Enter. Then enter the following value for the $user_vals context value where it says Enter value (annotated with red arrow in figure below).</p> <p></p> <p></p> <p></p> <p></p> <p>\"[[1,27,6,33,1,'CC','Budget','Standard',400,3,'M','S',0,46000,'Y',38,'NC',4444333322221111]]\"</p> <p>The final context variables window should look as shown in figure below with the $user_vals context variable defined and assigned the value above. Click the X to go back to the Try it out window.</p> <p></p> </li> <li> <p>Test the dialog flow in the Try it window by entering the following phrase:</p> <p>I would like to activate my device</p> <p>Note the response from Watson Assistant which corresponds to a \u201cT\u201d (true) churn prediction. To verify, click on Manage Context.</p> <p></p> <p>Tip</p> <p>On the context page, note the $webhook_result_1 context variable. This $webhook_result_1 is the context variable that captures the response to the webhook call which we had defined to call the Cloud Function which in turn calls the deployed churn prediction model.</p> <p></p> </li> <li> <p>Edit the $user_vals context variable by clicking on Manage Context, updating the values of $user_vals to the following values and then existing back to the Try it out window.</p> <p>\"[[1,27,6,33,1,'CC','Budget','Standard',400,3,'M','M',3,96000,'Y',38,'NC',4444333322221111]]\"</p> </li> <li> <p>Test the dialog flow again by entering the same phrase \u201cI would like to activate my device\u201d. The figure below shows the first response for the first set of values associated with user 1 (annotated with red oval) where the churn prediction is False as well as the second response for the second set of values associated with user 2 (annotated with red rectange) where the churn prediction is True.</p> <p>User 1 =&gt; values: \"[[1,27,6,33,1,'CC','Budget','Standard',400,3,'M','M',3,46000,'Y',38]]\"</p> <ul> <li>Churn Prediction: \u201cFalse\u201d</li> <li>Chatbot offers to assist with activating the user device</li> </ul> <p>User 2 =&gt; values: \"[[1,27,6,33,7,'CC','Budget','Standard',400,3,'M','S',0,96000,'Y',38]]\"</p> <ul> <li> <p>Churn Prediction: \u201cTrue\u201d</p> </li> <li> <p>Chatbot offers to transfer to a representative for a better end user experience.</p> </li> </ul> <p></p> </li> </ol>"},{"location":"demo-assistant/#conclusion","title":"Conclusion","text":"<p>In this lab, we illustrated how to infuse AI in a virtual assistant to personalize end user experience. We leveraged an AI model for churn prediction which was trained using AutoAI/notebook, deployed using Watson Machine Learning, and monitored for fairness, quality, drift using Watson OpenScale, all on Cloud Pak for Data.</p> <p>Then we infused that trained model in a virtual assistant with the help of cloud functions to call the REST endpoint for the ML model and to personalize the end user experience based on their likelihood to churn.</p> <p>The same approach can be leveraged for a variety of use cases across industries with even deeper personalization by including more end user features in the model and customizing the dialog interaction accordingly.</p> <p>For example, using a recommendation model, different users would see different products or offers being recommended based on their specific data.</p>"},{"location":"monitorai/","title":"Monitor AI Models","text":"<p>Info</p> <p>A key component of a Governed MLOps solution is the ability to monitor AI models for accuracy, fairness, explainability and drift. These capabilities deliver trustworthy AI which business leaders can safely adopt in their business processes and customer engagements.</p> <p>With multiple client engagements, we have found that having the confidence to trust AI models is just as important, and sometime even more important, than the performance of the AI models. Watson OpenScale, a component of Watson Studio and Cloud Pak for Data, is IBM\u2019s solution to deliver trustworthy AI and enable monitoring of AI models for fairness, explainability and drift.</p> <p>In this lab you will learn how to leverage Watson OpenScale to monitor the churn prediction model we previously deployed to Watson Machine Learning.</p>"},{"location":"monitorai/#db2-connection-details","title":"Db2 connection details","text":"<p>Later in this lab, you're going to provide OpenScale the training data of the model. Watson OpenScale supports reading training data from a CSV file, from Db2, and from Cloud Object Storage (COS). The training data in this workshop is stored in CP4D's Db2.</p> <p>Info</p> <p>Review Why does OpenScale need access to my training data for more details on why the training data is needed. Some clients prefer not to share their training data and OpenScale supports that by requesting the clients provide training data statistics information which they can obtain by running through a custom notebook that is available on github.</p> <ol> <li> <p>Go to menu Data &gt; Databases.</p> <p></p> </li> <li> <p>Click the menu of the Db2 database, then select Details.</p> <p></p> </li> <li> <p>Copy the Deployment id which you'll use in the next step. In this example, it's db2oltp-1712170077943241.</p> <p></p> </li> </ol>"},{"location":"monitorai/#enabling-monitoring-for-our-model","title":"Enabling monitoring for our model","text":"<ol> <li> <p>Go to menu Deployments:</p> <p></p> </li> <li> <p>Open the churnUATspace space where you deployed your initial AutoAI-generated Model for testing.</p> <p></p> </li> <li> <p>In the Deployments tab, open the user0_autoai_churn one. We're going to monitor this deployed model.</p> <p></p> </li> <li> <p>Go the Evaluations tab and click Configure OpenScale evaluation settings.</p> <p></p> <p>Warning</p> <p>If you see the following message, click Associate a server instance:</p> <p></p> <p>The following menu will appear. You're going to configure the monitors in the following section.</p> <p></p> </li> </ol>"},{"location":"monitorai/#training-data","title":"Training data","text":"<ol> <li> <p>Click on the pencil next to Training data. We're going to provide a connection to our training data.</p> <p></p> </li> <li> <p>Set the following options:</p> <ul> <li>Storage type: Database or cloud storage</li> <li>Location: Db2</li> <li>Credential values: Enter manually</li> <li>Hostname: c-&lt;YourDeploymentID&gt;-db2u-engn-svc (where &lt;YourDeploymentID&gt; is the Db2 Deployment ID you got in the previous step. In this example, the hostname would be c-db2oltp-1712170077943241-db2u-engn-svc)</li> <li>Use SSL: False</li> <li>Port: 50000</li> <li>Database: BLUDB</li> <li>Username: Your assigned CP4D username</li> <li>Password: Your assigned CP4D password</li> </ul> <p>Then click Connect.</p> <p></p> </li> <li> <p>Once the connection is succesful, select:</p> <ul> <li>Schema: CUSTOMER</li> <li>Table: CUSTOMER_TRAINING_DATA</li> </ul> <p>Then click Next</p> <p></p> </li> <li> <p>The label column was automatically selected. We're predicting CHURN. Click Next</p> <p></p> </li> <li> <p>For the feature columns, accept the default which selects all the features. Click Save.</p> <p></p> <p>The training data has been loaded to OpenScale.</p> </li> </ol>"},{"location":"monitorai/#configure-quality","title":"Configure Quality","text":"<p>Let's configure the Quality controls for our model.</p> <p>Info</p> <p>Watson OpenScale can monitor the Quality metric which measures the model\u2019s ability to correctly predict outcomes that match labeled data.</p> <ol> <li> <p>Select Quality from the Evaluations section. Then click the pencil icon.</p> <p></p> </li> <li> <p>Specify the Threshold value for Area under ROC to be 0.9 and click Next. This means that when the Area under ROC is less than 0.9, the quality monitor will flag an alert.</p> <p></p> </li> <li> <p>Change the Minimum sample size to 100. In production, you should use larger sample sizes to make sure they\u2019re representative of the requests the model receives. Click Save.</p> <p></p> </li> </ol>"},{"location":"monitorai/#configure-fairness","title":"Configure Fairness","text":"<p>Let's configure the Fairness controls for our model.</p> <p>Info</p> <p>In the fairness monitor, you specify to Watson OpenScale which features to monitor and what are the favourable outcomes. The Watson OpenScale fairness monitor determines whether outcomes that are produced by your model are fair or not for monitored group. When fairness monitoring is enabled, it generates a set of metrics every hour by default. You can generate these metrics on demand by clicking the\u00a0Check fairness now\u00a0button or by using the Python client.</p> <p>Watson OpenScale automatically identifies whether any known protected attributes are present in a model. When Watson OpenScale detects these attributes, it automatically recommends configuring bias monitors for each attribute present, to ensure that bias against these potentially sensitive attributes is tracked in production. </p> <p>Currently, Watson OpenScale detects and recommends monitors for the following protected attributes (sex, ethnicity, marital status, age, zip code).</p> <ol> <li> <p>Select Fairness in the Evaluations section and click the pencil icon to configure the fairness monitor.</p> <p></p> </li> <li> <p>Select Configure manually then click Next.</p> <p></p> </li> <li> <p>Next, select the favorable outcomes, specify F (false) as Favorable value (client is not going to churn), and T (true) as Unfavorable value. Then click Next.</p> <p></p> </li> <li> <p>Next, select the Minimum sample size to be 100. In production, you may want to select a larger sample size to make sure it is representative.  </p> <p></p> </li> <li> <p>Keep the default selections set to Disparate impact. Then Click Next</p> <p></p> </li> <li> <p>Leave the default values and click Next</p> <p></p> </li> <li> <p>OpenScale automatically detected the features GENDER and AGE as candidates for being monitored for fairness by analyzing the training data. Click Next.</p> <p></p> </li> <li> <p>For the AGE feature, you can specify the reference and monitored groups. Again, Watson OpenScale automatically recommends which group should be the reference and which group(s) should be monitored by analyzing the training data. Accept the default selections by Watson OpenScale. Click Next</p> <p></p> </li> <li> <p>Set the fairness alert threshold for AGE to 95 which effectively indicates that OpenScale will raise an alert when the model predicts a favorable outcome for the monitored group 95% of the times less than a favorable outcome for the reference group.</p> <p></p> <p>Info</p> <p>Check the How it works section in the documentation to better understand how fairness of the model is computed by OpenScale.</p> </li> <li> <p>Next, for the GENDER feature, specify the reference and monitored groups as F Monitored and M Reference. Then click Next.</p> <p></p> </li> <li> <p>Specify the fairness alert threshold to be 95 and click Save.</p> <p></p> <p>Optional</p> <p>You could also configure the Drift monitors. For details on Drift monitors, check the Drift documentation page.</p> </li> </ol>"},{"location":"monitorai/#configure-drift","title":"Configure Drift","text":"<ol> <li> <p>Go to the Drift v2 tab. Then click the pencil icon to configure it.</p> <p></p> </li> <li> <p>In this case, we'll use OpenScale directly to calculate possible Drift. Select Compute in Watson OpenScale and click Next.</p> <p></p> </li> <li> <p>Leave the default values. Feel free to read the description of these thresholds. Click Next.</p> <p></p> </li> <li> <p>In this case, all features will be considered important to detect drift. Select all features and click Next.</p> <p></p> </li> <li> <p>For the root cause analysis, select the following features:</p> <ul> <li>AGE</li> <li>ESTINCOME</li> <li>GENDER</li> <li>RATEPLAN</li> <li>USAGE</li> </ul> <p>Then click Next.</p> <p></p> </li> <li> <p>Click Save.</p> <p></p> </li> </ol>"},{"location":"monitorai/#configure-explainability","title":"Configure Explainability","text":"<ol> <li> <p>Go to the General settings option under the Explainability section. Then click the pencil icon to configure it.</p> <p></p> <p>Info</p> <p>Two different methods are available for explanations: Shapley Additive Explanations (SHAP) or Local Interpretable Model-agnostic Explanations (LIME).</p> <p>As described in the hint that appears when you click the Information box, SHAP often provides more thorough explanations, but LIME is faster.</p> </li> <li> <p>For this workshop, we'll use the LIME explanation method as it's faster. Click Next.</p> <p></p> </li> <li> <p>All features will be analyzed in this case. Click Save</p> <p></p> </li> </ol>"},{"location":"monitorai/#evaluating-the-model","title":"Evaluating the model","text":"<p>Now that the model monitors have been configured, you can run an evaluation of the model.</p> <ol> <li> <p>Click the X button to close the Configure monitors window.</p> <p></p> </li> <li> <p>Click on the Actions button then Evaluate now.</p> <p></p> </li> <li> <p>On the Import test data page, choose from CSV file and click Browse</p> <p></p> </li> <li> <p>Select the customer_churn_openscale_evaluation.csv file that you downloaded at the beginning of the workshop. Then click Upload and evaluate.</p> <p></p> <p>Info</p> <p>OpenScale will upload the data, run scoring against it and compare the model prediction to the labeled result to compute an overall quality score. It will also run the Fairness monitor to detect any fairness violations. Once the evaluation complete, you get a quick view in the dashboard of the Fairness and Quality results.</p> </li> <li> <p>Check the results of the evaluation. in the example below, it shows no alerts Quality, meaning the model meets or exceeds the required thresholds set for those monitors. However, there's an alert for Fairness and two alerts for Drift.</p> <p>Warning</p> <p>Note that your actual results may be different, and you may see different alerts.</p> <p></p> </li> </ol>"},{"location":"monitorai/#reviewing-fairness-and-explainability","title":"Reviewing Fairness and Explainability","text":"<ol> <li> <p>Click the arrow next to the Fairness monitor to review the fairness results further.</p> <p></p> </li> <li> <p>In this case, no bias was detected in the GENDER feature.</p> <p></p> </li> <li> <p>Click the Monitored attribute option and change it to the AGE feature.</p> <p></p> <p>Info</p> <p>In this case, a potential bias was found in the model against people in the first age group. Note that the thresholds we set in our monitors are key to decide when OpenScale is going to trigger alerts.</p> </li> <li> <p>Click the View payload transactions button.</p> <p></p> </li> <li> <p>These are the transactions that were evaluated. Choose one and click Explain prediction. In this example, we'll choose one with individual bias detected.</p> <p></p> </li> <li> <p>This graph explains how much each feature impacted the decision.</p> <p></p> <p>Go to tab Inspect.</p> <p></p> </li> <li> <p>In this section you can investigate how features should change for the prediction outcome to change. In the example below, this customer is predicted to churn (Predicted Outcom is T = True). Click Run analysis.</p> <p></p> </li> <li> <p>In this case, the STATUS feature should change from 'M' to 'S' for the model to predict no-churn instead of churn.</p> <p></p> </li> <li> <p>You can also investigate what the prediction would be if a specific feature changes to another value. For example, let's change the AGE from 46 to 18. Then, click Score new values to check what would be predicted in that case:</p> <p></p> </li> <li> <p>The prediction would change to no-churn in that case, with a confidence of 60%.</p> <p></p> </li> </ol>"},{"location":"monitorai/#reviewing-drift","title":"Reviewing Drift","text":"<ol> <li> <p>Go back to your deployment by clicking its name in the breadcrumb menu.</p> <p></p> </li> <li> <p>Go to the Evaluations tab then click the arrow in the Drift v2 section to open the drift test details.</p> <p></p> </li> <li> <p>Review the charts in this page. First, History chart.</p> <p></p> <p>Info</p> <p>This graph shows how the drift for the prediction (output drift), the incoming/evaluation data (feature drift), and the model quality has evolved in time. Only one evaluation was run in this case.</p> </li> <li> <p>Scroll down to the Output drift chart. Note that you can see drift charts for each predicted outcome by using the dropdown option:</p> <p></p> <p></p> <p>Info</p> <p>This graph shows how the prediction of the model has drifted from the training data to the input/evaluation data.</p> <p>In this case, we have a drift issue. We can investigate it further using the following graphs.</p> </li> <li> <p>Scroll down to the Feature drift chart.</p> <p></p> <p>Info</p> <p>This graph shows how the input/evaluated data has changed incomparison with the data that was used to train the model. This is useful to get an idea on which changes in data may have affected the model prediction accuracy, reducing analysis and investigation time.</p> <p>In this example, the feature LOCAL has significant changes.</p> </li> <li> <p>Scroll down to the following charts.</p> <p></p> <p></p> <p>Info</p> <p>For each feature with drift, the amount of changes for each range of values is shown. Besides, the evolution this feature's drift over time is shown in the second graph.</p> </li> </ol>"},{"location":"monitorai/#tracking-the-model-monitoring","title":"Tracking the model monitoring","text":"<ol> <li> <p>Go to the AI use cases menu.</p> <p></p> </li> <li> <p>Open your use case.</p> <p></p> </li> <li> <p>Go to the Lifecycle tab. Note that the AI Use Case has advanced from Testing to the Validation phase. The red icon shows that there are issues in the AI Monitors of this model. Clicking the red icon shows which monitors have issues.</p> <p></p> </li> <li> <p>Click the deployment name to open the details of the monitoring:</p> <p></p> </li> <li> <p>Take your time to review the results included in this phase.</p> <p></p> <p></p> <p></p> </li> </ol> <p>Info</p> <p>You've reached the end of this lab.</p>"},{"location":"pipelines/","title":"AI Pipelines","text":"<p>The Watson Pipelines editor provides a graphical interface for orchestrating an end-to-end flow of assets from creation through deployment. Assemble and configure a pipeline to create, train, deploy, and update machine learning models and Python scripts.</p> <p>Watson Pipelines is already configured once the service is installed.</p> <p>We're going to create a pipelines to refine data, train a model, choose the best model and deploy it in a deployment space. Through the use of pipelines, you can automate this process and parametrize to send different pipelines to cover several scenarios.</p>"},{"location":"pipelines/#pipeline-description","title":"Pipeline description","text":"<p>We are going to create a pipeline to run an AutoAI experiment and deploy to a deployment space. Steps of pipeline would be:</p> <ol> <li> <p>Run existing Data Refinery flow job (drjob) to create a csv with shaped data (customer_data_transactions_csv_shaped)</p> </li> <li> <p>Create an AutoAI experiment</p> </li> <li> <p>Run the AutoAI experiment and select the best model</p> </li> <li> <p>Promote the selected model to the a deployment space (existing dev deployment space)</p> </li> <li> <p>Deploy an online deployment based on the model selected.</p> </li> </ol> <p>You can see a diagram of the pipeline to create:</p> <p></p>"},{"location":"pipelines/#pipeline-creation","title":"Pipeline creation","text":"<ol> <li> <p>Open your project &lt;YourUser&gt; Customer Churn Prediction</p> </li> <li> <p>Click the New Asset+ button.</p> <p></p> </li> <li> <p>Then, select the asset type Pipelines:</p> <p></p> </li> <li> <p>Enter the name &lt;YourUser&gt; Customer Churn Pipeline and the description Pipeline to create and run an AutoAI experiment and deploy the best model to UAT deployment space. Then click Create.</p> <p></p> <p>Tip</p> <p>We don't select Datastage functions because this service is not installed</p> </li> </ol>"},{"location":"pipelines/#create-an-autoai-experiment","title":"Create an AutoAI experiment","text":"<ol> <li> <p>Drag a Create AutoAI experiment node to the canvas, then click on the three dots at right side of the node to expand options and click Open. The settings for this node will expand at the right hand side:</p> <p></p> </li> <li> <p>Set the following settings. Some settings will remain void to set by default options:</p> <ul> <li>AutoAI experiment name: Will fill in with the name \"Customer Churn pipeline experiment\"</li> <li>Select Scope is the project of space where the experiment will run. Select your project</li> <li>Prediction type is the type of experiment we want to run: Binary classification, Multiclass classification or Regression. We select \"Binary classification\". Note that Time Series experiments are created with other node type.</li> <li>Prediction column: \"CHURN\"</li> <li>Positive class: \"T\"</li> </ul> <p></p> <ul> <li>Training data split ratio: Leave 0.9 as by default value</li> <li>Algorithms to include: Algorithms to test during the AutoAI, leave it by default to take all of them into account</li> <li>Algorithms to use: How many top performers algorithms to use to create the subsequent pipelines, set it to 1 to reduce compute time</li> <li>Optimize metric: By default</li> <li>Hardware specification: You can select a hardware specification defined in the project or in any other location. Select the CPUx2 size: 2 vCPU 8GB RAM</li> </ul> <p></p> <ul> <li>AutoAI experiment description: \"AutoAI experiment to select best model to predict customer churn\"</li> <li>No tags</li> <li>Creation Mode: This is the action taken when the experiment finds another one with the same name: We select \"Overwrite\" to overwrite the existing AutoAI experiment.</li> </ul> <p>Then click Save to save this node's settings.</p> <p></p> </li> </ol>"},{"location":"pipelines/#running-a-data-refinery-job","title":"Running a Data Refinery job","text":"<ol> <li> <p>Drag a Run Data Refinery job node to the canvas then click on the three dots at right side of the node to expand its options. Click Open to open the node settings at the right hand side:</p> <p></p> </li> <li> <p>Set the following settings:</p> <ul> <li>Data Refinery Job: Select the job you created in the previous lab &lt;YourUser&gt; drjob, from the current project.</li> </ul> <p> </p> <ul> <li>Environment: Select the \"Default Data Refinery XS\" environment</li> <li>Don't override the default error policy, which states that pipeline will fail if error happens in this node.</li> </ul> <p>Then click Save to save this node's settings.</p> <p></p> </li> </ol>"},{"location":"pipelines/#running-the-autoai-experiment","title":"Running the AutoAI Experiment","text":"<ol> <li> <p>Drag a Run AutoAI experiment node to the canvas and connect it to the previous nodes as shown below.</p> <p>Info</p> <p>Note that links are not related to any data movement, but the sequence flow. The link between nodes notes the sequence of the process, and prevents the 2nd node to start before the 1st node has finished.</p> <p>In this particular case, the AutoAI experiment will start only if the AutoAI experiment has been created and DataRefinery job has finished, because it needs both elements to start the run.</p> <p></p> </li> <li> <p>Open the node. You could select any AutoAI experiment from your project, but in this case we're going to use the option from another node. This will use the experiment generated in the node \"Create AutoAI experiment\". Note that the type of result for this node shows as (\"AutoAI experiment\") when selecting it.</p> <p> </p> </li> <li> <p>Select the Training Data Asset to train the model. In this case, select the CSV file CUSTOMER_DATA_ready in your project. This is the file that's generated by the Data Refinery job.</p> <p> </p> </li> <li> <p>Check the rest of this node's properties:</p> <ul> <li>Don't select a holdout dataset</li> <li>Models Count: Number of best models to create. Set it to 1 to get the best one only.</li> <li>Run name: The name of each run, we leave it by default, it will define an automatic name.</li> <li>Model name prefix: Leave it by default. It will use \"&lt;experiment name&gt;-\"</li> <li>Run description: Insert the following description: \"AutoAI experiment for predicting the likelihood of a customer to churn with a training Dataset shaped from selected data\"</li> <li>Change the Creation Mode to \"Overwrite\", to allow overwriting models already created.</li> </ul> <p>Finally, click Save to save this node's settings.</p> <p></p> </li> </ol>"},{"location":"pipelines/#promote-the-generated-model","title":"Promote the generated model","text":"<p>The next step is to promote the model selected in the AutoAI experiment to a deployment space (\"churnUATspace\"). This is done with the \"Copy assets\" node.</p> <ol> <li> <p>Drag a Copy assets node to the canvas and connect it to the \"Run AutoAI experiment\" node as seen in the screenshot below. This connection is needed to get the model result of the previous node to use it in this node:</p> <p></p> </li> <li> <p>Open the node settings. The Source Asset will be the best model selected in the previous node. Choose the Select from another node option and select the \"Run AutoAI experiment\" node, and the Best model option.</p> <p> </p> <p>Tip</p> <p>Note that you could choose 2 options from an AI experiment: \"The best model\" to select the best model and \"Models\" to select all the models that were generated.</p> </li> <li> <p>For the Target, select the churnUATspace deployment space.</p> <p> </p> </li> <li> <p>Finally, select the Overwrite mode to overwrite the model in the space each time we run the pipeline. Then click Save to save this node's settings.</p> <p></p> </li> </ol>"},{"location":"pipelines/#deploy-the-model","title":"Deploy the model","text":"<p>The final step is to deploy an online model based on the model that was promoted to the deployment space.</p> <ol> <li> <p>Drag a Create online deployment node to the canvas and connect it to the \"Copy assets\" node as seen below. This connection is needed to get the model promoted to the deployment space in the previous step, and use it in this node.</p> <p></p> </li> <li> <p>Open the node settings. Set the option to Select from another node and select the \"Copy assets\" node, using the model that was promoted winning_model.</p> <p> </p> </li> <li> <p>Select the Overwrite option for \"Creation Mode\", to let it overwrite the deployment in case there is an existing one. Leave \"New deployment name\", \"New deployment description\" and \"New deployment tags\" by default. Then click Save</p> <p></p> </li> <li> <p>Finally, you can run the Pipeline as a Trial Run. This creates a trial job to run the pipeline. You could also create a job to schedule it or run it by demand.</p> <p></p> <p>For the Job run name, use &lt;YourUser&gt;_testrun. Then click Run.</p> <p></p> </li> <li> <p>During the run, you can inspect logs clicking on the node:</p> <p></p> </li> </ol> <p>Info</p> <p>You've reached the end of this lab.</p>"},{"location":"prereq1/","title":"Pre-requirements 1 - Environment Creation","text":"<p>Warning</p> <p>Ask your instructor if these pre-requirements have already been done for your workshop. You can skip this section in that case.</p>"},{"location":"prereq1/#create-the-cloud-pak-for-data-environment","title":"Create the Cloud Pak for Data Environment","text":"<p>To go through the workshop, you will need to have access to a Cloud Pak for Data environment with the following services installed:</p> <ul> <li> <p>Watson Studio</p> </li> <li> <p>Watson Machine Learning</p> </li> <li> <p>Watson Pipelines</p> </li> <li> <p>AI Factsheets</p> </li> <li> <p>Analytics Engine</p> </li> <li> <p>Data Management Console</p> </li> <li> <p>Db2</p> </li> </ul> <p>You can reserve such an environment from IBM TechZone. Use any of the following options to do so:</p> <ol> <li> <p>CP4D Pre-installed. Go to the Certified Collection for Pre-installed software. There, provision the environment CP4D 4.8.x - (no IAM) Base Installation that installs CP4D automatically. Once it's ready, you can add the required CP4D services by running the CP4D installation pipeline from the Openshift Console, section Pipelines.</p> </li> <li> <p>(Recommended) Using the CP4D Deployer. Go to the Certified Collection for VMWare on IBM Cloud. There, provision the environment Openshift VMWare Cluster - UPI - Deployer - V2. This environment will deploy Openshift with ODF storage, then you can use the CP4D Deployer to install CP4D automatically. This option is recommended becuase it allows you to install the latest version available automatically.</p> </li> <li> <p>Installing CP4D manually. Request the same environment as in the previous option. Then, use the Bastion Node available in the reservation to install CP4D yourself, following the CP4D Installation Documentation</p> </li> </ol> <p>In all cases, make sure you select:</p> <ul> <li>OCP version 4.14</li> <li>Storage ODF 2 TB</li> <li>5 Nodes</li> <li>Node Size: 16 vCPU with 300GB storage, or 32 vCPU with 300GB storage</li> </ul>"},{"location":"prereq1/#create-the-required-service-instances-and-databases","title":"Create the required service instances and databases","text":"<p>After all services are installed, you must create:</p> <ol> <li> <p>An instance of Db2 Data Management Console.</p> </li> <li> <p>An instance of Analytics Engine.</p> </li> <li> <p>The Data Refinery instance was already created in small. In order for all users to work concurrently, it must be changed to medium. From the Openshift console, open the DataRefinery custom resource named datarefinery-cr. In its YAML definition, change \"scaleConfig: small\" to \"scaleConfig: medium\" in the spec section.</p> </li> <li> <p>Once the Data Management Console instance is ready, create a Db2 Database named BLUDB. Use the default resource and storage sizes.</p> </li> </ol>"},{"location":"prereq2/","title":"Pre-requirements 2 - Environment Configuration","text":"<p>Warning</p> <p>Ask your instructor if these pre-requirements have already been done for your workshop. You can skip this section in that case.</p> <p>This document contains the details for initially configuring your Cloud Pak for Data environment so it's ready to follow this lab. Instructions assume you already have access to a Cloud Pak for Data environment as the admin user. If not, please follow the instructions in the Request an environment section.</p>"},{"location":"prereq2/#create-the-required-cp4d-role","title":"Create the required CP4D role","text":"<p>Info</p> <p>A new role will be created with the permissions the workshop attendees need.</p> <ol> <li> <p>Login to CP4D as the admin user.</p> <p></p> </li> <li> <p>Navigate to Access control by clicking on the navigation menu (top left hamburger menu) and then clicking Administration Access control.</p> <p></p> </li> <li> <p>Go to the Roles tab and click New role</p> <p></p> </li> <li> <p>Set the role name MLOps Workshop and add the following permissions:</p> <ul> <li>Catalogs &gt; Access Catalogs</li> <li>Deployments &gt; Create deployment spaces</li> <li>Deployments &gt; Monitor deployment activity</li> <li>Projects &gt; Create projects</li> <li>Projects &gt; Monitor project workloads</li> </ul> </li> <li> <p>Click Next then Create to create the new role.</p> <p></p> </li> </ol>"},{"location":"prereq2/#give-the-administrator-role-all-permissions","title":"Give the Administrator Role all permissions","text":"<ol> <li> <p>From the Access Control menu, go to tab Roles and open the Administrator Role.</p> <p></p> </li> <li> <p>Click Add Permissions +.</p> <p></p> </li> <li> <p>Select all available permissions, then click Add N permissions.</p> <p></p> </li> </ol>"},{"location":"prereq2/#create-the-required-users-in-cp4d","title":"Create the required users in CP4D","text":"<p>Info</p> <p>In this workshop, attendees will be sharing a single environment to optimize the available hardware resources.</p> <p>Create one user per attendee numbering them in ascending order, like this:</p> Full name Username Password Role user1 user1 user1 MLOps Workshop user2 user2 user2 MLOps Workshop user3 user3 user3 MLOps Workshop userX userX userX MLOps Workshop <p>The way you create users in CP4D will depend on the method you followed to deploy the environment in the section Request an Environment:</p> <ol> <li> <p>If you used the pre-installed environment or the CP4D deployer, login CP4D as the admin user, then go to Administration &gt; Access Control. You can create the users there.</p> <p>IMPORTANT</p> <p>Create your users with a Full Name. If they don't have a full name, AI Factsheets will fail when a new use case is created.</p> </li> <li> <p>If you installed CP4D manually, the IAM Service should be active. Therefore, add a new Openshift htpasswd-based Auth Provider with the workshop users. See this Red Hat's Documentation for information on how to do it. Login with each user in Openshift so they're active, then assign them the workshop role in CP4D Administration &gt; Access Control</p> </li> </ol>"},{"location":"prereq2/#data-upload","title":"Data Upload","text":"<p>In this section, you will upload the data in customer_training_data.csv to the Db2 database you created in section Request an Environment. Download the file to your local computer: customer_training_data.csv.</p> <ol> <li> <p>Login CP4D with the admin user.</p> </li> <li> <p>Navigate to Databases by clicking the Navigation menu and selecting Data &gt; Databases.</p> <p></p> </li> <li> <p>Find the database you created. Click the 3-dot menu and then Open database. If you get an error related to the Data Management Console, wait for some minutes then try again. If the page just doesn't load, try to open it in an incognito window.</p> <p></p> </li> <li> <p>Click the Summary drop down, expand Load drop down and select Load Data.</p> <p></p> </li> <li> <p>Click the browse files link and select the customer_training_data.csv file you downloaded before. Then click Next.</p> <p></p> </li> <li> <p>On the schema page, click New schema +, specify the schema name as CUSTOMER and click Create.</p> <p>Tip</p> <p>For future uploads of data sets to the CUSTOMER schema, you can select the CUSTOMER schema if it is already created.</p> <p></p> </li> <li> <p>On the Table page, click New table + (annotated with red oval), provide a table name as CUSTOMER_TRAINING_DATA (annotated with red rectangle) and click Create (annotated with red arrow).</p> <p></p> </li> <li> <p>Select the CUSTOMER_TRAINING_DATA as the table to upload data to (annotated with red rectangle) and click Next twice (annotated with red arrow).</p> <p></p> </li> <li> <p>Click Begin Load (annotated with red arrow) to begin loading the csv file into the CUSTOMER_TRAINING_DATA table on your Db2 instance which is provisioned on the same Cloud Pak for Data cluster.</p> <p></p> </li> <li> <p>The load operation will run for a minute or two and you can monitor progress through the steps.</p> <p></p> </li> <li> <p>When data is loaded, you should see a summary message indicating the data load job succeeded.</p> <p></p> </li> </ol>"},{"location":"prereq2/#platform-connection","title":"Platform connection","text":"<p>Create a Platform Connection named BLUDB that connects to your Db2 Database. You can use the internal Db2 service name for this:</p> <ol> <li> <p>To get the Db2 Deployment ID, go to the Databases menu Data &gt; Databases.</p> <p></p> </li> <li> <p>Find the database you created. Click the 3-dot menu and then Details.</p> <p></p> </li> <li> <p>Copy the Deployment ID, in this example it's db2oltp-1712170077943241.</p> <p></p> </li> <li> <p>Go to Menu &gt; Data &gt; Platform Connections.</p> <p></p> </li> <li> <p>Click New Connection +</p> <p></p> </li> <li> <p>Select the IBM Db2 connector and click Select</p> <p></p> </li> <li> <p>Insert the following values:</p> <ul> <li>Name: BLUDB</li> <li>Description: BLUDB Db2 Database</li> <li>Database: BLUDB</li> <li>Hostname: c-&lt;DeploymentID&gt;-db2u-engn-svc (&lt;DeploymentID&gt; being the Db2 ID you got before. In this example: c-db2oltp-1712170077943241-db2u-engn-svc)</li> <li>Port: 50000</li> <li>Username: admin (or your CP4D Administrator username)</li> <li>Password: admin's password</li> </ul> <p>Then click Test Connection to make sure the connection is successful. If it is, click Create</p> <p></p> </li> </ol>"},{"location":"prereq2/#db2-access-to-users","title":"Db2 access to users","text":"<ol> <li> <p>Go to your Db2 Database and select the option Manage access.</p> <p></p> </li> <li> <p>Add all workshop users with the role User.</p> </li> <li> <p>Now open the Database and go to Menu &gt; Run SQL.</p> <p></p> </li> <li> <p>Run the following SQL code. This will grant the workshops users access to the CUSTOMER training data.</p> <pre><code>GRANT SELECT ON CUSTOMER.CUSTOMER_TRAINING_DATA TO USER USER0;\nGRANT SELECT ON CUSTOMER.CUSTOMER_TRAINING_DATA TO USER USER1;\nGRANT SELECT ON CUSTOMER.CUSTOMER_TRAINING_DATA TO USER USER2;\nGRANT SELECT ON CUSTOMER.CUSTOMER_TRAINING_DATA TO USER USER3;\nGRANT SELECT ON CUSTOMER.CUSTOMER_TRAINING_DATA TO USER USER4;\nGRANT SELECT ON CUSTOMER.CUSTOMER_TRAINING_DATA TO USER USER5;\nGRANT SELECT ON CUSTOMER.CUSTOMER_TRAINING_DATA TO USER USER6;\nGRANT SELECT ON CUSTOMER.CUSTOMER_TRAINING_DATA TO USER USER7;\nGRANT SELECT ON CUSTOMER.CUSTOMER_TRAINING_DATA TO USER USER8;\nGRANT SELECT ON CUSTOMER.CUSTOMER_TRAINING_DATA TO USER USER9;\nGRANT SELECT ON CUSTOMER.CUSTOMER_TRAINING_DATA TO USER USER10;\nGRANT SELECT ON CUSTOMER.CUSTOMER_TRAINING_DATA TO USER USER11;\nGRANT SELECT ON CUSTOMER.CUSTOMER_TRAINING_DATA TO USER USER12;\nGRANT SELECT ON CUSTOMER.CUSTOMER_TRAINING_DATA TO USER USER13;\nGRANT SELECT ON CUSTOMER.CUSTOMER_TRAINING_DATA TO USER USER14;\nGRANT SELECT ON CUSTOMER.CUSTOMER_TRAINING_DATA TO USER USER15;\nGRANT SELECT ON CUSTOMER.CUSTOMER_TRAINING_DATA TO USER USER16;\nGRANT SELECT ON CUSTOMER.CUSTOMER_TRAINING_DATA TO USER USER17;\nGRANT SELECT ON CUSTOMER.CUSTOMER_TRAINING_DATA TO USER USER18;\nGRANT SELECT ON CUSTOMER.CUSTOMER_TRAINING_DATA TO USER USER19;\nGRANT SELECT ON CUSTOMER.CUSTOMER_TRAINING_DATA TO USER USER20;\n</code></pre> </li> </ol>"},{"location":"prereq2/#deployment-spaces","title":"Deployment spaces","text":"<ol> <li> <p>Create two new Deployment Spaces:</p> <ul> <li>Deployment space churnUATspace with Deployment Stage Testing</li> <li>Deployment space churn_prod_space with Deployment Stage Production</li> </ul> </li> <li> <p>Give the role Editor to the group \"All Users\" on both deployment spaces.</p> <p> </p> </li> </ol>"},{"location":"prereq2/#create-factsheets-inventory","title":"Create Factsheets inventory","text":"<ol> <li> <p>Go to menu AI Governance &gt; AI use cases.</p> <p></p> </li> <li> <p>Click the Manage settings button.</p> <p></p> </li> <li> <p>In the Inventories tab, click New inventory +.</p> <p></p> </li> <li> <p>Insert the name AI_inventory then click Next.</p> <p></p> </li> <li> <p>Click Add collaborators then Add user group.</p> <p></p> </li> <li> <p>Select the Editor role. Then search for the group All users and add it. Finally, click Add</p> <p></p> </li> <li> <p>Click the x button to close this window.</p> <p></p> </li> </ol>"},{"location":"prereq2/#watson-openscale-setup","title":"Watson OpenScale Setup","text":"<ol> <li> <p>Go to the menu Services &gt; Instances.</p> <p></p> </li> <li> <p>Find the openscale-defaultinstance, click the open drop down and select Manage access.</p> <p></p> </li> <li> <p>Click Add users + and add all workshop users with role Admin.</p> <p></p> </li> <li> <p>Go back to the list of Service Instances. Find the openscale-defaultinstance instance, click the open drop down and select Open.</p> <p></p> </li> <li> <p>Click Manual setup.</p> <p></p> </li> <li> <p>In the Database tab, click pencil icon to configure the database where model payload, predictions as well as calculated quality metrics will be stored.</p> <p></p> </li> <li> <p>Choose Db2 as database type and fill teh same Db2 credentials we used for the Platform Connection. Then click Connect.</p> <p></p> </li> <li> <p>For Schema, select Auto-create a new schema and click Save.</p> <p></p> </li> <li> <p>Go to the Machine learning providers tab and click Add machine learning provider.</p> <p></p> </li> <li> <p>Provide a Name WML and Description (Watson Machine Learning provider on Cloud Pak for Data) for the machine learning provider by clicking the pencil icons. Then click the pencil icon next to Connection.</p> <p></p> </li> <li> <p>For the Connection configuration, you need to specify a number of parameters for OpenScale to connect to the Watson Machine Learning as the provider:</p> <ul> <li>Service provider: Select Watson Machine Learning (V2) service</li> <li>Location: Local</li> <li>Deployment space: churnUATspace</li> <li>Environment type: Pre-production</li> </ul> <p>Then click Save.</p> <p></p> </li> <li> <p>Add another provider for the Production deployment space:</p> <ul> <li>Service provider: Select Watson Machine Learning (V2) service</li> <li>Location: Local</li> <li>Deployment space: churn_prod_space</li> <li>Environment type: Production</li> </ul> </li> </ol>"},{"location":"trainai/","title":"Training ML Models","text":"<p>In this lab, you will assume the role of a Data Scientist, who typically trains and evaluates AI models. The following data sets will be used, download them now:</p> <ul> <li> <p>customer_personal_info_simplified: This dataset captures personal information of the customers such as gender, marital status, income, age, and similar data.</p> </li> <li> <p>customer_data_transactions: This dataset captures the transaction data for customers.</p> </li> <li> <p>customer_churn_labels: This dataset captures information about whether a specific customer did churn or not.</p> </li> </ul> <p>Warning</p> <p>If you have issues downloading the data assets, try downloading them from the section Assets used, as these datasets are included there too.</p>"},{"location":"trainai/#creating-an-ai-use-case","title":"Creating an AI Use Case","text":"<p>The government of ML Models is managed from AI Use Cases. These AI Use Cases document the whole lifecycle of your models, from the initial planning to the monitoring of models in Production.</p> <p>This capability is provided by either the service FactSheets or watsonx.governance.</p> <ol> <li> <p>Go to menu AI Governance &gt; AI use cases.</p> <p></p> </li> <li> <p>Click New AI use case.</p> <p></p> </li> <li> <p>Insert the following information to create your AI Use Case for predicting your customers' churn:</p> <ul> <li>Name: \"&lt;YourUser&gt; Churn Prediction\" (for example: user0 Churn Prediction)</li> <li>Risk Level: Set whatever risk level you want for this use case</li> <li>Inventory: AI_inventory</li> <li>Purpose: \"Predict if customers are going to churn\"</li> <li>Status: \"Awaiting development\"</li> <li>Tags: \"Churn\"</li> </ul> <p>Info</p> <p>Note all the available values in Status. This field documents all the lifecycle stages</p> <p>Then click Create.</p> <p></p> </li> <li> <p>Your new AI Use Case has been created. All the deployment, training, promoting, and monitoring actions related to this use case will be documented here. Click Lifecycle.</p> <p></p> <p>Info</p> <p>You could create different approaches to this use case, and track your models in the approach each of them is using. In this workshop we'll just use the Default approach, so no additional action is needed.</p> <p></p> </li> </ol>"},{"location":"trainai/#data-preparation-with-data-refinery","title":"Data Preparation with Data Refinery","text":"<p>Before creating your ML model, you will prepare data using Data Refinery, a visual UI-based tool that enables users to interactively discover, cleanse and transform data with over 100 built-in operations.</p> <p>Info</p> <p>In IBM Cloud Pak for Data, a Project is how you organize your resources to achieve a particular goal. A project allows for high-level isolation, enabling users to package their project assets independently for different use cases or departments. Your project resources can include data, collaborators, scripts, and analytic assets like notebooks and models.</p> <ol> <li> <p>Log in CP4D with your assigned user. Contact your instructor if you haven't been assigned one.</p> </li> <li> <p>Go to the Projects section by clicking the Navigation menu and selecting Projects &gt; All projects.</p> <p></p> </li> <li> <p>Click on New project to create a new project.</p> <p></p> </li> <li> <p>Select the Create an empty project option.</p> <p></p> </li> <li> <p>Insert the following Project Name: &lt;YourUser&gt; Customer Churn Prediction where  is your username. For example: \"user0 Customer Churn Prediction\". Then Click Create.</p> <p></p> </li> <li> <p>Now that the project is created, the Data Scientist will shop for relevant data assets that can help them with training this AI model for customer churn prediction. Click the Find and add data icon.</p> <p>Info</p> <p>In a real case, the Data Scientist would search for relevant assets in CP4D Catalogs. However, for simplicity sake, we'll use the 3 datasets referenced earlier.</p> <p></p> </li> <li> <p>Either drag and drop or browse the customer_personal_info_simplified.csv file that you downloaded into the Drop files here or browse box. Wait for the file to upload to the project. Then, repeat it for the other csv files:</p> <ul> <li>customer_data_transactions.csv</li> <li>customer_churn_labels.csv</li> </ul> <p></p> </li> <li> <p>Verify the 3 CSV files have been uploaded to your project by clicking the Assets tab. You should see all 3 files under the Data assets section:</p> <ul> <li>customer_data_transactions.csv</li> <li>customer_churn_labels.csv</li> <li>customer_personal_info_simplified.csv</li> </ul> <p></p> </li> <li> <p>Next step is to shape the data to get it ready to be used for training ML models.</p> <p>Info</p> <p>Cloud Pak for Data supports multiple approaches for data shaping and transformation. In this lab, you will use Data Refinery to cleanse and shape the data with a graphical flow editor and create a joined data set of the Customer Data asset and the labeled churn data set.</p> <p>Click New Asset and select Data Refinery.</p> <p></p> <p></p> <p>Tip</p> <p>Alternatively, you could access Data Refinery by clicking on the open and close list of options menu (3 vertical dots) next to a data asset in your project and then selecting Refine.</p> </li> <li> <p>Select Data asset and then select the customer_data_transactions.csv data set and click Select.</p> <p></p> </li> <li> <p>This loads the dataset in Data Refinery. Note the tabs Data, Profile, and Visualizations.</p> <p>Warning</p> <p>If the Profile and Visualizations tabs are grayed out, wait for some time as the asset is being loaded.</p> <ul> <li> <p>The Data tab displays the data and enables you to apply a number of common operations to cleanse and shape the data in a graphical editor. It also supports deploying R library operations, functions, and logical operators via the command line.</p> </li> <li> <p>The Profile tab shows useful summary statistics including a histogram of each of the data fields. This is useful to understand the statistical distribution of the data as well as potential skew that may exist.</p> </li> <li> <p>The Visualizations tab provides over 20 customizable charts to provide perspective and insights into the data.</p> </li> </ul> <p></p> </li> <li> <p>Change the ID column type from Integer to String. This is needed because in the next step when you apply a join of this data and the other data sets, need the column types to match. To do so, click on the 3-dot menu next to the ID column, select Convert column type and then select String type and click Apply.</p> <p></p> <p></p> </li> <li> <p>Next, you will add a step to join this data set with the customer_personal_info_simplified.csv dataset to capture additional features that may impact the likelihood of a customer to churn. Click the New Step button which will open the operations column, scroll down to find the Join operation and click Join. You can also type Join in the Search operations field and it will filter the list of operations to find Join.</p> <p></p> <p></p> </li> <li> <p>On the Join operations window, keep the type of join as \"Left join\" then click Add data set.</p> <p></p> </li> <li> <p>On the Data set page, click Data asset, select the customer_personal_info_simplified.csv dataset and click Apply. </p> <p>Tip</p> <p>In case you\u2019re not familiar with the Left Join operation, Data Refinery provides an explanation of what that operation does; specifically, a Left Join returns all rows in the original data set and returns only the matching rows in the joining dataset.</p> <p></p> <p>Back on the Join operation window, click Select column to specify ID as the field to use for joining the two data sets. Then click Next.</p> <p></p> <p></p> </li> <li> <p>On the next window, it shows all the fields that will result from the join operation. At this point, you can remove fields you do not wish to include in the final data set. For this lab, keep all the fields selected and click Apply.</p> <p></p> </li> <li> <p>Repeat the process (previous 3 steps) to apply a join operation on the resulting data set and the customer_churn_labels.csv dataset. The data set to join is customer_churn_labels.csv, and the join field is ID.</p> <p></p> </li> <li> <p>Note that the Data Refinery flow has been augmented with all the executed operations. As you perform more operations to shape the data, they get added to the Data Refinery flow. For this lab, we will just perform the Join operations but typically, you\u2019d perform several other operations to transform the data and make it ready for analytics insights and training machine learning models.</p> <p></p> <p>Info</p> <p>In a real-case scenario, data typically requires several more operations to cleanse by removing nulls, filtering rows with missing data, aggregating data across fields, and/or applying a number of different operations. In this lab, the dataset we\u2019re using is already in good shape and the only operations you will apply is to join the customer data (which was already a join of customer personal information and transaction data) and labeled churn data set.</p> <p>Take a minute to browse the set of supported operations.</p> <p>Tip</p> <p>This video and this tutorial provide more examples and details of operations for data transformation using Data Refinery.</p> </li> <li> <p>Change the output file name. To do so, click the Settings button.</p> <p></p> </li> <li> <p>Go to the tab Target data set. Then click Edit properties.</p> <p></p> </li> <li> <p>In the Data asset name field, insert CUSTOMER_DATA_ready. Then click Save and Apply.</p> <p> </p> </li> <li> <p>Once you\u2019ve applied all the operations to transform the data and configured the output, next step is to save the flow and create a job to apply this data refinery flow against the complete data set. To do so, click on Save and create a job.</p> <p></p> </li> <li> <p>Provide a Name for the job &lt;YourUser&gt; drjob, and click Next.</p> <p></p> <p>On the Configure tab, review the Environment and keep the default selection Default Data Refinery XS, then click Next. For jobs that require more resources, you can select a larger Environment to run the job.</p> <p></p> <p>On the Schedule tab, keep the Schedule slider set to off, and click Next. In this lab, we don\u2019t need to run the data refinery job at a given schedule but we\u2019ll manually run it as needed and that is why we kept the default selection as off.</p> <p></p> <p>On the Notify tab, keep the Notification off as default. Click Next.</p> <p></p> <p>On the Review and create tab, review the job details and click Create and run.</p> <p></p> </li> <li> <p>After you click Create and Run, navigate to the jobs view to monitor progress by clicking on the Navigation menu and selecting Jobs.</p> <p></p> </li> <li> <p>On the Jobs page, you can filter the view by selecting whether you want to look at Active runs, Jobs with active runs, Jobs with finished runs, or Finished runs. Feel free to filter the different views to see the results. Initially, the job will appear in the view Jobs with active runs and when it completes, the job will appear in the view Jobs with finished runs. Feel free to click on the job name and review the details and status of the run(s).</p> <p></p> </li> <li> <p>Navigate back to the project and click the Assets tab. Note the Data Refinery flow, customer_data_transactions.csv_flow which is now a project asset and the newly created data asset, CUSTOMER_DATA_ready.csv, which was created by running the Data Refinery flow that joined the customer data transactions with the customer personal information and churn labels data sets. Click the CUSTOMER_DATA_ready.csv (red arrow) asset to review the data.</p> <p></p> </li> </ol> <p>Info</p> <p>At this point, you have collected data from various sources and leveraged Data Refinery to shape the data using a graphical editor. Now, the data is ready to be used for training a machine learning model for predicting the likelihood of a customer to churn based on his/her demographic and transaction data.</p>"},{"location":"trainai/#train-autoai-model-for-churn-prediction","title":"Train AutoAI Model for Churn Prediction","text":"<p>In this section, we illustrate how to leverage AutoAI to quickly train multiple AI models for churn prediction and select the pipeline that delivers best performance.</p> <ol> <li> <p>Navigate back to your project and click Assets tab, click New asset + and select AutoAI.</p> <p></p> <p></p> </li> <li> <p>Provide the following name to your AutoAI experiment: &lt;YourUser&gt;_autoai_churn_prediction, keep the default Compute configuration and click Create.</p> <p>Tip</p> <p>You could select a different configuration if you needed to assign more resources for your AutoAI experiment.</p> <p></p> </li> <li> <p>On the AutoAI add data sources page, click the Select from project button since you will be using the dataset you had created earlier with Data Refinery.</p> <p>Tip</p> <p>You could also click Browse to upload data from your local machine.</p> <p></p> </li> <li> <p>Click Data asset and select the checkbox to select the CUSTOMER_DATA_ready dataset. Then click Select asset.</p> <p></p> </li> <li> <p>On the next page, you will see the selected dataset and you will be prompted to select whether you want to Create a time series forecast which is supported by AutoAI. Click No since customer churn prediction is a classification use case and not a time series forecasting use case.</p> <p>Once you click No, you will get the option to select which column to predict. Scroll down the list to select the CHURN column. At this point, we have provided the data set, indicated it is a classification use case and selected the prediction column. Click Run experiment (blue button) to kick off the AutoAI run.</p> <p>Tip</p> <p>Note that you can click the Experiment settings to review the default settings and change some of the configurations if you wish. Review those settings as they\u2019re very informative.</p> <p></p> </li> <li> <p>AutoAI runs for a few minutes on this dataset and produces a number of pipelines as shown in figure below including training/test data split, data preprocessing, feature engineering, model selection, and hyperparameter optimization. You can dig deeper into any of the pipelines to better understand feature importance, the resulting metrics, the selected model, and any applied feature transformation.</p> <p>Tip</p> <p>While waiting for AutoAI\u2019s run to complete, you could review the AutoAI Documentation.</p> <p>Specifically, review the AutoAI Implementation Details to understand what algorithms are supported, what data transformations are applied and what metrics can be optimized.</p> <p> </p> <p>The AutoAI run will take 4-5 minutes to complete. Once complete, please spend a few minutes exploring the dashboard:</p> <ul> <li> <p>Switch between the Experiment details and the Legend information (annotated with red rectangles in previous figure) to better understand the generated Relationship map.</p> </li> <li> <p>Switch between Cross Validation and Holdout results by clicking the icon next to Cross Validation (annotated with red oval in previous figure) to see how the pipeline ranking changed depending on which data is being evaluated.</p> </li> <li> <p>Swap the view between the Relationship map and the Progress map (annotated with blue oval in previous figure) to see the different views of the AutoAI pipeline creation process.</p> </li> <li> <p>Click on the top pipeline (annotated with blue rectangle in previous figure) to review the details for that pipeline. AutoAI reports several valuable evaluation criteria like several performance metrics (Accuracy, Area under ROC, Precision, Recall, F1) as well asthe confusion matrix, Precision Recall Curve, and feature importance. If the pipeline also included feature engineering (or feature transformation), the pipeline details will explain what transformations were applied. Close the pipeline details window by clicking x top right of window.</p> </li> </ul> <p>After reviewing the trained pipelines, you can decide which one you\u2019d like to save as a model to deploy. Assuming you select the first pipeline, mouse over the first pipeline and click Save as (annotated with red arrow in figure above).</p> </li> <li> <p>On the Save As page, select Model, keep the default name, and click Create.</p> <p>Tip</p> <p>Note that you could also save the pipeline as a Notebook which you can customize further.</p> <p></p> </li> </ol>"},{"location":"trainai/#track-your-model-in-the-ai-use-case","title":"Track your Model in the AI use case","text":"<ol> <li> <p>Navigate back to the project assets by clicking your project's name and then clicking the Assets tab.</p> <p>Click on the name of your saved model &lt;YourUser&gt;_autoai_churn_prediction - P5 Extra Trees Classifier - Model to open it.</p> <p></p> </li> <li> <p>Feel free to check all the information available now; as the lifecycle continues, more sections will be populated.</p> <p>Info</p> <p>This view contains all the governance information of your model. It includes information about the use case, how the model was trained, which data was used to train it, the evaluations of the model, the monitoring alerts...</p> <p></p> </li> <li> <p>Click Track in AI use case.</p> <p></p> </li> <li> <p>Select your AI Use Case then click Next.</p> <p></p> </li> <li> <p>You'll use the Default approach. You could've created different approaches for the use case. Click Next.</p> <p></p> </li> <li> <p>You can set a version number to this specific model. In this case, it's not been tested yet, so select version 0.0.1. Then click Next</p> <p></p> </li> <li> <p>Click Track asset.</p> <p></p> <p>Info</p> <p>The information of your model indicates that the model is in the Development phase, as it has not been deployed for testing yet.</p> <p></p> </li> <li> <p>Check your AI Use Case. Go to menu AI Governance &gt; AI use cases*.</p> <p></p> </li> <li> <p>Then open your use case.</p> <p></p> </li> <li> <p>Go to the Lifecycle tab. This tab shows which assets are being used in each stage of the model's lifecycle. Note that your trained model has been added to the Develop step. Besides, all available model versions are shown in this view.</p> <p></p> </li> </ol>"},{"location":"trainai/#deploy-your-model-for-testing","title":"Deploy your Model for testing","text":"<p>Let's promote the trained model to a UAT (Testing) Deployment Space, then deploy it so it's ready to be called for testing.</p> <ol> <li> <p>Navigate back to the project assets by clicking your project's name and then clicking the Assets tab.</p> <p>Click on the menu of your saved model &lt;YourUser&gt;_autoai_churn_prediction - P5 Extra Trees Classifier - Model then click Promote to space. Note that the name of your model may be different.</p> <p>Warning</p> <p>Note that the name of your model may be different. Even a different classifier may have been used in your case.</p> <p></p> <p>Info</p> <p>Deployment spaces allow you to create deployments for machine learning models and functions and view and manage all of the activity and assets for the deployments, including data connections and connected data assets.</p> <p>A deployment space is not associated with a project. You can deploy assets from multiple projects to a space, and you can deploy assets to more than one space. For example, you might have a Test space for evaluating deployments, and a Production space for deployments you want to deploy in business applications.</p> </li> <li> <p>On the Promote to space page, select the churnUATspace from Target Space drop-down. Keep the default selected version (Current) and click Promote.</p> <p></p> </li> <li> <p>Once the model is successfully promoted to the deployment space, you will see a notification message. Click on the deployment space link to navigate to the deployment space.</p> <p></p> </li> <li> <p>On the churnUATspace deployment space page, your AutoAI model &lt;YourUser&gt;_autoai_churn_prediction \u2013 P7 XGBClassifier is ready to be deployed for consumption. Click on the 3-dot menu next to the model then click Deploy.</p> <p></p> </li> <li> <p>On the Create a deployment page, select Online, add the Name &lt;YourUser&gt;_autoai_churn and click Create.</p> <p></p> </li> <li> <p>Click on the Deployments tab and wait until the deployment status changes to Deployed. Then click on the deployed model name &lt;YourUser&gt;_autoai_churn.</p> <p></p> </li> <li> <p>On the model page API reference tab, review the model Endpoint and the various Code snippets (in different coding languages) to illustrate how to make an API call to the deployed model. Then select the Test tab, click on the Provide input data as JSON icon, paste the following JSON sample in the Enter input data window and click Predict (blue button).</p> </li> </ol> <p>IMPORTANT</p> <p>Use the copy button in the code snippet below to keep the right format of the content when pasting.</p> <pre><code>{\n\"input_data\": [\n{\n\"fields\":\n[\"ID\",\"LONGDISTANCE\",\"INTERNATIONAL\",\"LOCAL\",\"DROPPED\",\"PAYMETHOD\",\"LOCALBILLTYPE\",\"LONGDISTANCEBILLTYPE\",\"USAGE\",\"RATEPLAN\",\"GENDER\",\"STATUS\",\"CHILDREN\",\"ESTINCOME\",\"CAROWNER\",\"AGE\"],\n\"values\":[[1,28,0,60,0,\"Auto\",\"FreeLocal\",\"Standard\",89,4,\"F\",\"M\",1,23000,\"N\",45]]\n}\n]\n}\n</code></pre> <p> </p> <p>Info</p> <p>The deployed model will predict the likelihood of the user to churn given the specific values for the various features. The model returns the predicted churn label as \u201cT\u201d (true) or \u201cF\u201d (false) and the probability of that prediction which effectively expresses the likelihood of that user to churn (or not). A \u201cT\u201d label returned by the model indicates the user is likely to churn and the corresponding probability.</p> <p>These probabilities can be used in conjunction with the predicted label to better serve customers on a more granular basis. Your application can be customized to make decisions based on the predicted label and the probabilities of that prediction.</p> <p>IMPORTANT</p> <p>There's a known issue in the current version of the workshop environment that prevents the AI Use Case from automatically register that the model was deployed in a Test Space. Follow this workaround:</p> <ol> <li> <p>Close the prediction results window by clicking the x button:</p> <p></p> </li> <li> <p>Then go back to the Deployment Space using the breadcrumb menu:</p> <p></p> </li> <li> <p>Click your model to open it:</p> <p></p> </li> <li> <p>Go to the AI Factsheet tab then click Untrack:</p> <p></p> </li> <li> <p>Click Untrack:</p> <p></p> </li> <li> <p>Now click Track in AI use case to enable tracking again:</p> <p></p> </li> <li> <p>Choose your AI Use Case and follow the same steps as you did before. Now, the Test step should be enabled because a Testing Deployment was detected:</p> <p></p> </li> </ol>"},{"location":"trainai/#track-your-deployment","title":"Track your deployment","text":"<ol> <li> <p>Go to the AI use cases menu.</p> <p></p> </li> <li> <p>Open your use case.</p> <p></p> </li> <li> <p>Go to the Lifecycle tab. Note that the AI Use Case has advanced to the Test step. The available Test deployments are shown in their Deployment Space:</p> <p></p> </li> </ol>"},{"location":"trainai/#train-churn-prediction-model-with-jupyter-notebook","title":"Train Churn Prediction Model with Jupyter Notebook","text":"<p>In this section, we illustrate an alternate method for training AI models in Cloud Pak for Data, namely by using Jupyter notebook and open-source libraries. This is a very common and mostly preferred method by data scientists as it provides them with the utmost flexibility in exploring different algorithms for training best performing AI models.</p> <ol> <li> <p>Go back to your project. Then click New asset + and select Jupyter notebook editor.</p> <p></p> </li> <li> <p>On the New notebook page, go to the Local file tab and click the Drag and drop files here or upload. Select the notebook that you downloaded before churn_prediction_pyspark.ipynb to upload it. Then, select the runtime Default Spark 3.4 &amp; Python 3.10 (we will be using PySpark to run this Notebook on CP4D). Finally, click Create.</p> <p></p> <p>IMPORTANT</p> <p>DO NOT RUN THE NOTEBOOK YET A change is needed first to avoid all users from overiding other users' work.</p> </li> <li> <p>DO NOT RUN THE NOTEBOOK YET. First, review the notebook comments to get an idea of what the Notebook is going to do:</p> <p>Notebook Steps</p> <ol> <li> <p>Access the CUSTOMER_DATA_ready dataset from your project.</p> </li> <li> <p>Process the data to prepare features relevant for the prediction.</p> </li> <li> <p>Train a Random Forest ML model to predict the likelihood of customers to churn using a sample of the data.</p> </li> <li> <p>Evaluate the model against test data not used in training.</p> </li> <li> <p>Create the Deployment Space churnUATspace if it doesn't exist.</p> </li> <li> <p>Associate Watson Machine Learning with the churnUATspace deployment space.</p> </li> <li> <p>Store the model in the churnUATspace deployment space</p> </li> <li> <p>Deploy the model in the churnUATspace deployment space.</p> </li> <li> <p>Run a test to validate the online deployment of the model.</p> </li> </ol> </li> <li> <p>Go to the following lines and add your username to the MODEL_NAME and DEPLOYMENT_NAME variables:</p> <ul> <li>MODEL_NAME = \"&lt;YourUser&gt; Churn Model\"</li> <li>DEPLOYMENT_NAME = \"&lt;YourUser&gt; Churn Deployment\"</li> </ul> <pre><code># Provide a target name for your churn model\nMODEL_NAME = \"&lt;YourUser&gt; Churn Model\"\n# Provide a target name for your churn model deployment\nDEPLOYMENT_NAME = \"&lt;YourUser&gt; Churn Deployment\"\n</code></pre> <p>For example, for user0:</p> <p></p> </li> <li> <p>Now, run all the cells and review the output of each one.</p> </li> <li> <p>If all cells ran successfully, your model should be deployed. Go to the Menu Deployments.</p> <p></p> </li> <li> <p>On the Deployments page, go to the tab Spaces and click the churnUATspace space.</p> <p></p> </li> <li> <p>Go to tab Deployments. The model that you trained and deployed from the Jupyter Notebook is available for consumption.</p> <p></p> </li> </ol> <p>Info</p> <p>You've reached the end of this lab.</p>"}]}